<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 20px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 100px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
/* 
img {
 width:900px;
}
*/
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
<xmp>
This is personal study note
Copyright and original reference:
https://www.youtube.com/watch?v=O1U2NWaSYn4&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz

================================================================================
There is no application with only forward probability and backward probability

================================================================================
For decoding problem
  - Viterbi decoding algorithm


================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_07_29_15:36:46.png' alt=''><xmp>

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:11:06.png' alt=''><xmp>

Forward probability $$$\times$$$ backward probability
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:13:43.png' alt=''><xmp>

================================================================================
Why did you create forward and backward probabilities?

For representing 
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:14:31.png' alt=''><xmp>

It is joint probability between
prob of latent factor z in k-th cluster at time t and prob of X

================================================================================
Forward and backward probabilities can be written in detail (recursive form, repeating problem)
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:16:30.png' alt=''><xmp>

$$$\alpha_t^k$$$ reduces $$$\alpha_{t-1}^i$$$
And you use parameter a and b (emission and transition probabilities)

$$$\beta_{t+1}^i$$$: start with $$$t+1$$$, so it's also reduction in size from $$$\beta_{t}^k$$$
And you also use a and b

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:20:17.png' alt=''><xmp>

This becomes joint prob of "latent variable wrt specific time t" and X

================================================================================
Joint prob means you can use conditional prob

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:22:02.png' alt=''><xmp>

Problem:
you observe all Xs
But observations are used for one z

But you want to assign most probable assignment into all hidden labels Zs
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_07_29_15:41:15.png' alt=''><xmp>

================================================================================
So, this is lack for above ultimate goal
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:23:39.png' alt=''><xmp>

This is called "decoding question"

================================================================================
Viterbi decoding algorithm

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:24:34.png' alt=''><xmp>

$$$k^*$$$: most probable assignment of specific cluster k
$$$P(z^k=1|X)$$$: all observations Xs are given, prob of $$$z^k$$$ ocurring

If you configure normalize term well, you can make following
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:27:14.png' alt=''><xmp>

================================================================================
Now, you need to model entire sequence of Z

How to do it? Use dynamic programming

Fill Z values from left to right by using memorization tables

Perform retrace and get most probable likelihood Z sequence
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:29:25.png' alt=''><xmp>

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:30:26.png' alt=''><xmp>

See 
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:30:43.png' alt=''><xmp>
$$$x_1,\cdots,x_{t-1}$$$: all observations up to time $$$t-1$$$
$$$z_1,\cdots,z_{t-1}$$$: all latent variables up to time $$$t-1$$$
$$$x_t$$$: observation at time t
$$$z_t^k=1$$$: your goal to do, let's assign z into specific cluster k

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:33:20.png' alt=''><xmp>

Change z values to maximize P

You get maximal prob P, and it's $$$V_t^k$$$

================================================================================
Do process

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:35:46.png' alt=''><xmp>

================================================================================
See this

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:37:04.png' alt=''><xmp>

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:37:23.png' alt=''><xmp>

$$$V_t^k$$$ becomes "repeating problem form"

You can use dynamic programming for "repeating problem form"

================================================================================
Car assembly line in the factory

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:39:10.png' alt=''><xmp>

left 2,4: starting point
right 3,2: ending point

Each number means took-time in minutes

When time is predicted too much, line can be transfered from A to B or B to A

================================================================================
Which routes would be the best?

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:42:05.png' alt=''><xmp>

2+7=9
4+8=12

================================================================================
Find optimial route

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:44:52.png' alt=''><xmp>

================================================================================
Let's above process in terms of probability

You want to know $$$V_t^k$$$

$$$V_t^k$$$ is assignment by performing joint of time and states

Let's save 2 information of $$$V_t^k$$$ 

1. Trace information
2. probabilities information

================================================================================
Initialize Viterbi algorithm

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:52:09.png' alt=''><xmp>

Peform iteration to growth V

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:53:28.png' alt=''><xmp>
You can calculate $$$V_t^k$$$ from $$$V_{t-1}^i$$$

You already know parameters a and b

================================================================================
Which assignment is the best?

Information on it is saved in trace

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:54:56.png' alt=''><xmp>

================================================================================
You access T (entire sequence) from t

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:55:58.png' alt=''><xmp>

================================================================================
Issue of Viterbi decoding algorith

* Frequent underflow problem
In real world, time step is large

But following a,b,V are all probabilities [0,1]
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/mnic/basic/009_Hidden_markov_model/004_Viterbi_Decoding_Algorithm/pics/2019_05_20_10:57:49.png' alt=''><xmp>
Small values are multipled continuously

* To resolve it, calculate it in log domain
And multiplication is converted into summation

================================================================================
Summary

When $$$\pi,a,b,X$$$ are given
inference whole latent factor sequence Z? using Viterbi decoding algorithm

</xmp>
   </BODY>
</HTML>
