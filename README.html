<h1>Machine Learning Theory</h1>

JJ - Outline concept of a pattern recognition<br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/01_002.html">01_002_Process_of_pattern_recognition</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/01_003.html">01_003_Feature_Feature_vector_Feature_space_Pattern_Scatter_plot</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_001_Introduce_vector_in_linear_algebra.html">02_001_Introduce_vector_in_linear_algebra</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_002_Operation_on_vector.html">02_002_Operation_on_vector</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_003_Orthogonal_projection.html">02_003_Orthogonal_projection</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_004_Linear_combination_Linearly_dependent_Linearly_independent.html">02_004_Linear_combination_Linearly_dependent_Linearly_independent</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_005_Basis_vector_Vector_space.html">02_005_Basis_vector_Vector_space</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_006_Vector_space_Euclid_space_Distance_between_2_vectors_in_Euclidian_distance.html">02_006_Vector_space_Euclid_space_Distance_between_2_vectors_in_Euclidian_distance</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_001_Transpose_matrix_Squre_matrix_Diagonal_matrix_Scalar_matrix_Identity_matrix_Symmetry_matrix_Orthogonal_matrix.html">03_001_Transpose_matrix_Squre_matrix_Diagonal_matrix_Scalar_matrix_Identity_matrix_Symmetry_matrix_Orthogonal_matrix</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_002_Trace_of_matrix_Determinant_value_of_matrix.html">03_002_Trace_of_matrix_Determinant_value_of_matrix</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_003_Inverse_matrix_Positive-definite_Positive-semidefinite_matrix.html">03_003_Inverse_matrix_Positive-definite_Positive-semidefinite_matrix</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_004_Eigenvector_Eigenvalue.html">03_004_Eigenvector_Eigenvalue</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_005_Linear_transform.html">03_005_Linear_transform</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-04">10-04 Nonparametric density estimation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-05">10-05 Nonparametric density estimation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-06">10-06 Nonparametric density estimation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-07">10-07 Nonparametric density estimation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-01">11-01 Clustring</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-02">11-02 Clustring</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-03">11-03 Clustring</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-04">11-04 Clustring</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-01">12-01 Dimentionality reduction by PCA(principle component analysis)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-02">12-02 Dimentionality reduction by PCA(principle component analysis)</a><br/>
12-03 Short video 1 for the eigenvector and eigenvalue<br/>  
12-04 Short video 2 for the eigenvector and eigenvalue<br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-05">12-05 Dimentionality reduction by PCA(principle component analysis)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-06">12-06 Dimentionality reduction by PCA(principle component analysis)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-01">13-01 LDA(linear discriminant analysis)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-02">13-02 LDA(linear discriminant analysis)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-03">13-03 LDA(linear discriminant analysis)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-04">13-04 LDA(linear discriminant analysis)</a><br/>

<br/><br/>
TAcademy - Machine Learning Concepts<br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/04">04. Decision Tree</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/05">05. Suppor Vector Machine(SVM)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/10">10. Recurrent Neural Net(RNN) </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/12">12. Example applications with an applied deep learning</a><br/>

<br/><br/>
CWLee - Deep learning concept<br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/02">02. Regression and gradient descent</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/03">03. Gradient Descent & Normal Equation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/04">04. Logistic Regression</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/05">05. Loss function in logistic regression</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/06">06. Implementing a neural net(NN)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/07">07. Backpropagation in neural net</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/08">08. softmax function</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/09">09. Convolutional Neural Net(CNN)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/10">10. CNN Back Propagation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/11">11. Local Minima</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/12">12. Unsupervised Pre-training for CNN</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/13">13. RNN Introduction</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/14">14. Back Propagation in RNN</a><br/>

<br/><br/>
HWLee - Outline concept of a random process <br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/01-01">01-01. Probability axioms and random variables </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/01-02">01-02. Probability axioms and random variables </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/02-01">02-01. Function of random variables, Definitions of convergence, Convergence in probability, Convergence with probability 1, Convergence in distribution </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/02-02">02-02. Function of random variables, Definitions of convergence, Convergence in probability, Convergence with probability 1, Convergence in distribution </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/03-01">03-01. Useful inequalities and law of large numbers. Central limit theorem. Markov inequality. Chebyshev inequality. Chernoff bound. </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/03-02">03-01. Useful inequalities and law of large numbers. Central limit theorem. Markov inequality. Chebyshev inequality. Chernoff bound. </a><br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes<br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes<br/>
05-01. Discrete-time Markov chains and steady-state behavior Definition, state transition probability, Markov property<br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review<br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review<br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times<br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times<br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula<br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula<br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization<br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization<br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/10-01">10-01. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models</a><br/>
10-02. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models<br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis<br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis<br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms<br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms<br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms<br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms <br/>

<br/><br/>
<h2>dohkim - Data Science</h2>
<strong>2. Math for data science</strong><br>
(05) Probability theory</br>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/005_Probability_theory/001_Set_theory">001_Set_theory</a><br/>
(06) Probability distribution</br>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/006_Probability_distribution/016_Dirichlet_distribution">016_Dirichlet_distribution</a><br/>
(04) Optimization using SciPy</br>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/004_Optimization_using_scipy/001_Basic_of_optimization">001_Basic_of_optimization</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/004_Optimization_using_scipy/002_Optimization_with_constraint">002_Optimization_with_constraint</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/004_Optimization_using_scipy/003_LP_problem_QP_problem">003_LP_problem_QP_problem</a><br/>
(07) Correlation relationship<br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/007_Correlation_relationship/001_Multivariate_discrete_random_variable">001_Multivariate_discrete_random_variable</a><br/>
<strong>4. Machine learning for data science</strong><br/>
(14) Probabilistic graph model<br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/014_Probabilistic_graph_model/001_Graph_based_probabilistic_model">001_Graph_based_probabilistic_model</a><br/>

<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-001">008-001. introduction to pandas</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-002">008-002. ID data, load csv, create csv, export csv</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-003">008-003. dataframe indexer, loc[], iloc[], at[], iat[]</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-003">008-004. dataframe manipulating data</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/010-001">010-001. nltk package for natural language processing</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/025-001">025-001. bernoulli distribution</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/031-001">031-001. meaning of test and parameter estimation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/031-002">031-002. testing and p-value</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/045-001">045-001. K-Means clustring</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/048-002">048-002. naive bayes classification model</a><br/>

<br/><br/>
icmoon - Machine Learning Basic<br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/001">001. Week 01. Motivations and Basics - 01. Motivation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/002">002. Week 01. Motivations and Basics - 02. MLE(maximum likelihood estimation)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/003">003. Week 01. Motivations and Basics - 03. MAP(maximum posteriori estimation)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/004">004. Week 01. Motivations and Basics - 04. Probability and Distribution</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/005">005. Week 02. Fundamentals of Machine Learning - 01. Rule-Based machine learning</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/008">008. Week 02. Fundamentals of Machine Learning - 04. Entropy and Info. Gain</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/009">009. Week 02. Fundamentals of Machine Learning - 05. How to create a Disicion tree</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/010">010. Week 03. Naive Bayes Classifier - 01. Optimal Classification</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/011">011. Week 03. Naive Bayes Classifier - 02. Conditional Independence</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/018">018. Week 04. Logistic Regression - 05. How Gradient method works</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/061">061. Week 10. Sampling Based Inference - 01. Forward Sampling</a><br/>
2 Fundamentals of Machine Learning | Lecture 2 Intro. to Rule-Based<br/>
2 Fundamentals of Machine Learning | Lecture 3 Introduction to Decision Tree<br/>
3 Naive Bayes Classifier | Lecture 3 Naive Bayes Classifier<br/>
<br/>
3 Naive Bayes Classifier | Lecture 4 Naive Bayes Classifier with Matlab<br/>
<br/>
4 Logistic Regression | Lecture 1 Decision Boundary<br/>
<br/>
4 Logistic Regression | Lecture 2 Introduction to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 3 Logistic Regression Parameter Approximation 1<br/>
<br/>
4 Logistic Regression | Lecture 4 Gradient method<br/>
<br/>
4 Logistic Regression | Lecture 5 How Gradient method works<br/>
<br/>
4 Logistic Regression | Lecture 6 Logistic Regression Parameter Approximation 2<br/>
<br/>
4 Logistic Regression | Lecture 7 Naive Bayes to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 8 Naive Bayes vs Logistic Regression<br/>
<br/>
5 Support Vector Machine | Lecture 1 Decision boundary with Margin<br/>
<br/>
5 Support Vector Machine | Lecture 2 Maximizing the Margin<br/>
<br/>
5 Support Vector Machine | Lecture 3 SVM with Matlab<br/>
<br/>
5 Support Vector Machine | Lecture 4 Error Handling in SVM<br/>
<br/>
5 Support Vector Machine | Lecture 5 Soft Margin with SVM<br/>
<br/>
5 Support Vector Machine | Lecture 6 Rethinking of SVM<br/>
<br/>
5 Support Vector Machine | Lecture 7 Primal, Dual with KKT Condition<br/>
<br/>
5 Support Vector Machine | Lecture 8 Kernel<br/>
<br/>
5 Support Vector Machine | Lecture 9 SVM with Kernel<br/>
<br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/032">032. Week 06. Training, Testing, Regularization - 01. Overfitting, Underfitting</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/033">033. Week 06. Training, Testing, Regularization - 02. Trade-off relation between bias and variance</a><br/>
<br/>
6 Training Testing and Regularization | Lecture 3 Occam’s razor<br/>
<br/>
6 Training Testing and Regularization | Lecture 4 Cross Validation<br/>
<br/>
6 Training Testing and Regularization | Lecture 5 Performance Metrics<br/>
<br/>
6 Training Testing and Regularization | Lecture 6 Regularization<br/>
<br/>
6 Training Testing and Regularization | Lecture 7 Regularization Approximation<br/>
<br/>
7 Bayesian Network | Lecture 1 Probability Concepts<br/>
<br/>
7 Bayesian Network | Lecture 2 Probability Theorems<br/>
<br/>
7 Bayesian Network | Lecture 3 Interpretation of Bayesian Network<br/>
<br/>
7 Bayesian Network | Lecture 4 Bayes Ball Algorithm<br/>
<br/>
7 Bayesian Network | Lecture 5 Factorization of Bayesian networks<br/>
<br/>
7 Bayesian Network | Lecture 6 Inference Question on B. Networks<br/>
<br/>
7 Bayesian Network | Lecture 7 Variable Elimination<br/>
<br/>
7 Bayesian Network | Lecture 8 Potential Function and Clique Graph<br/>
<br/>
7 Bayesian Network | Lecture 9 Potential Function and Clique Graph<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 1 K-Means 1<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 2 K-Means 2<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 3 Multi<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 4 Multivar.<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 5 G.M.M<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 6 EM(Elimination-Maximization) step<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 7 Relation<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 8 EM(Elimination-Maximization)<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 9 Deriv. EM<br/>
<br/>
9 Hidden Markov Model | Lecture 1 Concept of Hidden Markov Model<br/>
<br/>
9 Hidden Markov Model | Lecture 2 Joint, Marginal Probability of HMM<br/>
<br/>
9 Hidden Markov Model | Lecture 3 Forward-Backward Probability Calculation<br/>
<br/>
9 Hidden Markov Model | Lecture 4 Viterbi Decoding Algorithm<br/>
<br/>
9 Hidden Markov Model | Lecture 5 Baum-Welch Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 1 Forward Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 2 Rejection Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 3 Importance Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 4 Markov Chain<br/>
<br/>
10 Sampling Based Inference | Lecture 5 Markov Chain for Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 6 Metropolis-Hastings Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 7 Gibbs Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 8 Understand the LDA(Latent Dirichlet Allocation)<br/>
<br/>
10 Sampling Based Inference | Lecture 9 Gibbs Sampling for LDA (1)<br/>
<br/>
10 Sampling Based Inference | Lecture 10 Gibbs Sampling for LDA (2)<br/>

<br/><br/>
Todo<br/>
ICMoon - Machine Learning Advanced<br/>
1 Variational Inference | Lecture 1<br/>
<br/>
1 Variational Inference | Lecture 2<br/>
<br/>
1 Variational Inference | Lecture 3<br/>
<br/>
1 Variational Inference | Lecture 4<br/>
<br/>
1 Variational Inference | Lecture 5<br/>
<br/>
1 Variational Inference | Lecture 6<br/>
<br/>
1 Variational Inference | Lecture 7<br/>
<br/>
1 Variational Inference | Lecture 8<br/>
<br/>
1 Variational Inference | Lecture 9<br/>
<br/>
1 Variational Inference | Lecture 10<br/>
<br/>
1 Variational Inference | Lecture 11<br/>
<br/>
1 Variational Inference | Lecture 12<br/>
<br/>
1 Variational Inference | Lecture 13<br/>
<br/>
1 Variational Inference | Lecture 14<br/>
<br/>
1 Variational Inference | Lecture 15<br/>
<br/>
1 Variational Inference | Lecture 16<br/>
<br/>
1 Variational Inference | Lecture 17<br/>
<br/>
1 Variational Inference | Lecture 18<br/>
<br/>
1 Variational Inference | Lecture 19<br/>
<br/>
1 Variational Inference | Lecture 20<br/>
<br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/advanced/002_001">2 Dirhichlet Process | Lecture 1</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/advanced/002_002">2 Dirhichlet Process | Lecture 2</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/advanced/002_003">2 Dirhichlet Process | Lecture 3</a><br/>
<br/>
2 Dirhichlet Process | Lecture 4<br/>
<br/>
2 Dirhichlet Process | Lecture 5<br/>
<br/>
2 Dirhichlet Process | Lecture 6<br/>
<br/>
2 Dirhichlet Process | Lecture 7<br/>
<br/>
2 Dirhichlet Process | Lecture 8<br/>
<br/>
2 Dirhichlet Process | Lecture 9<br/>
<br/>
2 Dirhichlet Process | Lecture 10<br/>
<br/>
2 Dirhichlet Process | Lecture 11<br/>
<br/>
2 Dirhichlet Process | Lecture 12<br/>
<br/>
2 Dirhichlet Process | Lecture 13<br/>
<br/>
2 Dirhichlet Process | Lecture 14<br/>
<br/>
2 Dirhichlet Process | Lecture 15<br/>
<br/>
2 Dirhichlet Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 1<br/>
<br/>
3 Gaussian Process | Lecture 2<br/>
<br/>
3 Gaussian Process | Lecture 3<br/>
<br/>
3 Gaussian Process | Lecture 4<br/>
<br/>
3 Gaussian Process | Lecture 5<br/>
<br/>
3 Gaussian Process | Lecture 6<br/>
<br/>
3 Gaussian Process | Lecture 7<br/>
<br/>
3 Gaussian Process | Lecture 8<br/>
<br/>
3 Gaussian Process | Lecture 9<br/>
<br/>
3 Gaussian Process | Lecture 10<br/>
<br/>
3 Gaussian Process | Lecture 11<br/>
<br/>
3 Gaussian Process | Lecture 12<br/>
<br/>
3 Gaussian Process | Lecture 13<br/>
<br/>
3 Gaussian Process | Lecture 14<br/>
<br/>
3 Gaussian Process | Lecture 15<br/>
<br/>
3 Gaussian Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 17<br/>
<br/>
3 Gaussian Process | Lecture 18<br/>
<br/>
3 Gaussian Process | Lecture 19<br/>
<br/>
3 Gaussian Process | Lecture 20<br/>
<br/>
3 Gaussian Process | Lecture 21<br/>
<br/>
4 Artificial Neural Network | Lecture 1<br/>
<br/>
4 Artificial Neural Network | Lecture 2<br/>
<br/>
4 Artificial Neural Network | Lecture 3<br/>
<br/>
4 Artificial Neural Network | Lecture 4<br/>
<br/>
4 Artificial Neural Network | Lecture 5<br/>
<br/>
4 Artificial Neural Network | Lecture 6<br/>
<br/>
4 Artificial Neural Network | Lecture 7<br/>
<br/>
4 Artificial Neural Network | Lecture 8<br/>

<br/><br/>
