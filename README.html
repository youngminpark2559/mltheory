<h1>Machine Learning Theory</h1>

================================================================================<br/>
<h1>JJ - Outline concept of a pattern recognition</h1>
http://www.kocw.net/home/search/kemView.do?kemId=1189957<br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/jjpr/01_002.html">01_002_Process_of_pattern_recognition</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/01_003.html">01_003_Feature_Feature_vector_Feature_space_Pattern_Scatter_plot</a><br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_001_Introduce_vector_in_linear_algebra.html">02_001_Introduce_vector_in_linear_algebra</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_002_Operation_on_vector.html">02_002_Operation_on_vector</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_003_Orthogonal_projection.html">02_003_Orthogonal_projection</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_004_Linear_combination_Linearly_dependent_Linearly_independent.html">02_004_Linear_combination_Linearly_dependent_Linearly_independent</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_005_Basis_vector_Vector_space.html">02_005_Basis_vector_Vector_space</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/02_006_Vector_space_Euclid_space_Distance_between_2_vectors_in_Euclidian_distance.html">02_006_Vector_space_Euclid_space_Distance_between_2_vectors_in_Euclidian_distance</a><br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_001_Transpose_matrix_Squre_matrix_Diagonal_matrix_Scalar_matrix_Identity_matrix_Symmetry_matrix_Orthogonal_matrix.html">03_001_Transpose_matrix_Squre_matrix_Diagonal_matrix_Scalar_matrix_Identity_matrix_Symmetry_matrix_Orthogonal_matrix</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_002_Trace_of_matrix_Determinant_value_of_matrix.html">03_002_Trace_of_matrix_Determinant_value_of_matrix</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_003_Inverse_matrix_Positive-definite_Positive-semidefinite_matrix.html">03_003_Inverse_matrix_Positive-definite_Positive-semidefinite_matrix</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_004_Eigenvector_Eigenvalue.html">03_004_Eigenvector_Eigenvalue</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/jjpr/03_005_Linear_transform.html">03_005_Linear_transform</a><br/><br/>

005. Probability and statistics<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/005_Probability_and_statistics1/001_Population_Sample_Sampling_distribution_Mean_Variance_Covariance_Correlation_coefficient.html">001_Population_Sample_Sampling_distribution_Mean_Variance_Covariance_Correlation_coefficient</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/005_Probability_and_statistics1/003.html">003</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/005_Probability_and_statistics1/004_Probability_space_Axioms_on_probability.html">004_Probability_space_Axioms_on_probability</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/005_Probability_and_statistics1/005_Marginal_probability_Conditional_probability_Joint_probability_Independent_trial_Law_of_the_total_probability.html">005_Marginal_probability_Conditional_probability_Joint_probability_Independent_trial_Law_of_the_total_probability</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/005_Probability_and_statistics1/006_Bayes_theorem.html">006_Bayes_theorem</a><br/><br/>

006. Random variable, Probability distribution<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/006_Random_variable_Probability_distribution/001.html">001</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/006_Random_variable_Probability_distribution/002.html">002</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/006_Random_variable_Probability_distribution/003.html">003</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/006_Random_variable_Probability_distribution/004.html">004</a><br/><br/>

007. Bayesian Decision Theory<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/007_Bayesian_Decision_Theory/001.html">001 Key points:</a><br/>
&nbsp&nbsp- Discrete random variable</br>
&nbsp&nbsp- Cumulitive distribution function</br>
&nbsp&nbsp- Probability mass function</br>
&nbsp&nbsp- Probability density function</br>
&nbsp&nbsp- Univariate gaussian probability density function</br>
&nbsp&nbsp- Multivariate gaussian probability density function</br>
&nbsp&nbsp- Covariance</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/007_Bayesian_Decision_Theory/002.html">002 Key points:</a><br/>
&nbsp&nbsp- Normal distribution</br>
&nbsp&nbsp- Central limit theorem</br>
&nbsp&nbsp- The reason that gaussian distribution is much used</br>
&nbsp&nbsp- Relationship between pattern of covariance and data's distribution</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/007_Bayesian_Decision_Theory/004.html">004 Key points:</a><br/>
&nbsp&nbsp- Statistical methods</br>
&nbsp&nbsp- Likelihood ratio test</br>
&nbsp&nbsp- Maximum likelihood estimation</br>
&nbsp&nbsp- Likelihood</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/007_Bayesian_Decision_Theory/005_Probability_of_error.html">005_Probability_of_error</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/007_Bayesian_Decision_Theory/006_Bayes_risk.html">006_Bayes_risk</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/007_Bayesian_Decision_Theory/007_Summary_3_variants_of_LRT_Bayes_criterion_MAP_criterion_ML_criterion.html">007_Summary_3_variants_of_LRT_Bayes_criterion_MAP_criterion_ML_criterion</a><br/><br/>

008. Bayesian Decision Theory with multiple classes<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/008_Bayesian_Decision_Theory_with_multiple_classes/001_Use_multi_classes_on_MAP_Bayes_criterion_considering_cost.html">001_Use_multi_classes_on_MAP_Bayes_criterion_considering_cost</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/008_Bayesian_Decision_Theory_with_multiple_classes/002_How_to_predict_likelihood_pdf_Parameter_estimation_Non_parameter_density_function_Maximum_likelihood_estimation.html">002_How_to_predict_likelihood_pdf_Parameter_estimation_Non_parameter_density_function_Maximum_likelihood_estimation</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/008_Bayesian_Decision_Theory_with_multiple_classes/003.html">003</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/008_Bayesian_Decision_Theory_with_multiple_classes/004.html">004</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/008_Bayesian_Decision_Theory_with_multiple_classes/005.html">005</a><br/><br/>

009. Quadratic classifier<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/009_Quadratic_classifier/001.html">001 Key points:</a><br/>
&nbsp&nbsp- Various shapes of covariance matrix in Gaussian distribution function</br></br><br/>

010. <br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-04">10-04 Nonparametric density estimation</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-05">10-05 Nonparametric density estimation</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-06">10-06 Nonparametric density estimation</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/10-07">10-07 Nonparametric density estimation</a><br/><br/>

011. <br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-01">11-01 Clustring</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-02">11-02 Clustring</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-03">11-03 Clustring</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/11-04">11-04 Clustring</a><br/><br/>

012. <br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-01">12-01 Dimentionality reduction by PCA(principle component analysis)</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-02">12-02 Dimentionality reduction by PCA(principle component analysis)</a><br/>
&nbsp&nbsp12-03 Short video 1 for the eigenvector and eigenvalue<br/>  
&nbsp&nbsp12-04 Short video 2 for the eigenvector and eigenvalue<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-05">12-05 Dimentionality reduction by PCA(principle component analysis)</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/12-06">12-06 Dimentionality reduction by PCA(principle component analysis)</a><br/><br/>

013. <br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-01">13-01 LDA(linear discriminant analysis)</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-02">13-02 LDA(linear discriminant analysis)</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-03">13-03 LDA(linear discriminant analysis)</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/jjpr/13-04">13-04 LDA(linear discriminant analysis)</a><br/>

<br/><br/>
================================================================================<br/>
<h1>TAcademy - Machine Learning Concepts</h1>
https://www.youtube.com/playlist?list=PL9mhQYIlKEheuxhyGbUIpKR1EFM-o0Br1<br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/TAcademy/04">04. Decision Tree</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/05">05. Suppor Vector Machine(SVM)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/10">10. Recurrent Neural Net(RNN) </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/TAcademy/12">12. Example applications with an applied deep learning</a><br/>

<br/><br/>
================================================================================<br/>
<h1>CWLee - Deep learning concept</h1>
https://www.youtube.com/playlist?list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc<br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/cwlee/02">02. Regression and gradient descent</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/03">03. Gradient Descent & Normal Equation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/04">04. Logistic Regression</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/05">05. Loss function in logistic regression</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/06">06. Implementing a neural net(NN)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/07">07. Backpropagation in neural net</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/08">08. softmax function</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/09">09. Convolutional Neural Net(CNN)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/10">10. CNN Back Propagation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/11">11. Local Minima</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/12">12. Unsupervised Pre-training for CNN</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/13">13. RNN Introduction</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/cwlee/14">14. Back Propagation in RNN</a><br/>

<br/><br/>
================================================================================<br/>
<h1>HWLee - Outline concept of a random process</h1>
http://www.kocw.net/home/search/kemView.do?kemId=991018<br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/hwlee/01-01">01-01. Probability axioms and random variables </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/01-02">01-02. Probability axioms and random variables </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/02-01">02-01. Function of random variables, Definitions of convergence, Convergence in probability, Convergence with probability 1, Convergence in distribution </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/02-02">02-02. Function of random variables, Definitions of convergence, Convergence in probability, Convergence with probability 1, Convergence in distribution </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/03-01">03-01. Useful inequalities and law of large numbers. Central limit theorem. Markov inequality. Chebyshev inequality. Chernoff bound. </a><br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/03-02">03-01. Useful inequalities and law of large numbers. Central limit theorem. Markov inequality. Chebyshev inequality. Chernoff bound. </a><br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes<br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes<br/>
05-01. Discrete-time Markov chains and steady-state behavior Definition, state transition probability, Markov property<br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review<br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review<br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times<br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times<br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula<br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula<br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization<br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization<br/>
<a href="https://youngminpark2559.github.io/mltheory/hwlee/10-01">10-01. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models</a><br/>
10-02. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models<br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis<br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis<br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms<br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms<br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms<br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms <br/>

<br/><br/>
================================================================================<br/>
<h1>DHKim - Data Science</h1>
https://datascienceschool.net<br/><br/>

<strong>2. 003_Regression ananalysis and Timeseries analysis</strong><br>
(01) Introduction to package and dataset for regression analysis</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/003_Regression_ananalysis_and_Timeseries_analysis/001_Introduction_to_package_and_dataset_for_regression_analysis/01.01_statsmodels_package/main.html">01.01_statsmodels_package</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/003_Regression_ananalysis_and_Timeseries_analysis/001_Introduction_to_package_and_dataset_for_regression_analysis/01.02_scikit-learn_package/main.html">01.02_scikit-learn_package</a><br/><br/>
(02) Basic of linear regression</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/003_Regression_ananalysis_and_Timeseries_analysis/002_Basic_of_linear_regression/02.01_Basic_of_linear_regression/main.html">02.01_Basic_of_linear_regression</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/003_Regression_ananalysis_and_Timeseries_analysis/002_Basic_of_linear_regression/02.03_Range_datatype_independent_variable/main.html">02.03_Range_datatype_independent_variable</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/003_Regression_ananalysis_and_Timeseries_analysis/002_Basic_of_linear_regression/02.05_Geometric_perspective_on_linear_regression_analysis/main.html">02.05_Geometric_perspective_on_linear_regression_analysis</a><br/><br/>

<strong>2. Math for data science</strong><br>
(04) Optimization using SciPy</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/004_Optimization_using_scipy/001_Basic_of_optimization">001_Basic_of_optimization</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/004_Optimization_using_scipy/002_Optimization_with_constraint">002_Optimization_with_constraint</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/004_Optimization_using_scipy/003_LP_problem_QP_problem">003_LP_problem_QP_problem</a><br/>
(05) Probability theory</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/005_Probability_theory/001_Set_theory">001_Set_theory</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/005_Probability_theory/002_Mathematical_definition_of_probability_and_its_meaning">002_Mathematical_definition_of_probability_and_its_meaning</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/005_Probability_theory/003_Probability_distribution_function">003_Probability_distribution_function</a><br/>
(06) Probability distribution</br>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/006_Probability_distribution/016_Dirichlet_distribution">016_Dirichlet_distribution</a><br/>
(07) Correlation relationship<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/007_Correlation_relationship/001_Multivariate_discrete_random_variable.html">001_Multivariate_discrete_random_variable</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/007_Correlation_relationship/002_Multivariate_continuous_random_variable.html">002_Multivariate_continuous_random_variable</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/007_Correlation_relationship/003_Independant_and_correlation_of_random_variables.html">003_Independant_and_correlation_of_random_variables</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/007_Correlation_relationship/004_Covariance_and_correlation_coefficient.html">004_Covariance_and_correlation_coefficient</a><br/>
(08) Test and inference<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/002_Math_for_data_science/008_Test_and_inference/005_MLE_mechanism.html">005_MLE_mechanism</a><br/></br>

<strong>4. Machine learning for data science</strong><br/>
(15) Probabilistic graph model<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/014_Probabilistic_graph_model/14.01_Graph_theory/main.html">14.01_Graph_theory</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/014_Probabilistic_graph_model/14.02_Probabilistic_graph_model/main.html">14.02_Probabilistic_graph_model</a><br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/014_Probabilistic_graph_model/14.03_Network_inference/main.html">14.03_Network_inference</a><br/>
(16) State space model<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/016_State_space_model/16.01_Hidden_markov_model/main.html">16.01_Hidden_markov_model</a><br/>
(17) Monte Carlo<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/015_Monte_Carlo/001_Monte_Carlo_Bayesian_analysis.html">001_Monte_Carlo_Bayesian_analysis</a><br/>
(18) Mixture model and Variational inference<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/dohkim/004_Machine_learning_for_data_science/016_State_space_model/16.01_Hidden_markov_model/main.html">18.01_Hidden_markov_model</a><br/><br/>

<br/><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-001">008-001. introduction to pandas</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-002">008-002. ID data, load csv, create csv, export csv</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-003">008-003. dataframe indexer, loc[], iloc[], at[], iat[]</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/008-003">008-004. dataframe manipulating data</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/010-001">010-001. nltk package for natural language processing</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/025-001">025-001. bernoulli distribution</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/031-001">031-001. meaning of test and parameter estimation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/031-002">031-002. testing and p-value</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/045-001">045-001. K-Means clustring</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/dohkim/048-002">048-002. naive bayes classification model</a><br/>

<br/><br/>
================================================================================<br/>
<h1>Terry - Deep Learning Topics</h1>
https://www.youtube.com/playlist?list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq<br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/terry/YouTube/001_005_Metrics_for_deep_learrning_classification_Accuracy_Precision_Recall.html">001_005_Metrics_for_deep_learrning_classification_Accuracy_Precision_Recall</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/terry/YouTube/001_006_ROC_curve_AUC_Precision_Recall.html">001_006_ROC_curve_AUC_Precision_Recall</a><br/>

<br/><br/>
================================================================================<br/>
<h1>icmoon - Machine Learning Basic</h1>
https://www.youtube.com/playlist?list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz<br/><br/>

<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/001">001. Week 01. Motivations and Basics - 01. Motivation</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/002">002. Week 01. Motivations and Basics - 02. MLE(maximum likelihood estimation)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/003">003. Week 01. Motivations and Basics - 03. MAP(maximum posteriori estimation)</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/004">004. Week 01. Motivations and Basics - 04. Probability and Distribution</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/005">005. Week 02. Fundamentals of Machine Learning - 01. Rule-Based machine learning</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/008">008. Week 02. Fundamentals of Machine Learning - 04. Entropy and Info. Gain</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/009">009. Week 02. Fundamentals of Machine Learning - 05. How to create a Disicion tree</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/010">010. Week 03. Naive Bayes Classifier - 01. Optimal Classification</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/011">011. Week 03. Naive Bayes Classifier - 02. Conditional Independence</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/018">018. Week 04. Logistic Regression - 05. How Gradient method works</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/061">061. Week 10. Sampling Based Inference - 01. Forward Sampling</a><br/>
2 Fundamentals of Machine Learning | Lecture 2 Intro. to Rule-Based<br/>
2 Fundamentals of Machine Learning | Lecture 3 Introduction to Decision Tree<br/>
3 Naive Bayes Classifier | Lecture 3 Naive Bayes Classifier<br/>
<br/>
3 Naive Bayes Classifier | Lecture 4 Naive Bayes Classifier with Matlab<br/>
<br/>
4 Logistic Regression | Lecture 1 Decision Boundary<br/>
<br/>
4 Logistic Regression | Lecture 2 Introduction to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 3 Logistic Regression Parameter Approximation 1<br/>
<br/>
4 Logistic Regression | Lecture 4 Gradient method<br/>
<br/>
4 Logistic Regression | Lecture 5 How Gradient method works<br/>
<br/>
4 Logistic Regression | Lecture 6 Logistic Regression Parameter Approximation 2<br/>
<br/>
4 Logistic Regression | Lecture 7 Naive Bayes to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 8 Naive Bayes vs Logistic Regression<br/>
<br/>
5 Support Vector Machine | Lecture 1 Decision boundary with Margin<br/>
<br/>
5 Support Vector Machine | Lecture 2 Maximizing the Margin<br/>
<br/>
5 Support Vector Machine | Lecture 3 SVM with Matlab<br/>
<br/>
5 Support Vector Machine | Lecture 4 Error Handling in SVM<br/>
<br/>
5 Support Vector Machine | Lecture 5 Soft Margin with SVM<br/>
<br/>
5 Support Vector Machine | Lecture 6 Rethinking of SVM<br/>
<br/>
5 Support Vector Machine | Lecture 7 Primal, Dual with KKT Condition<br/>
<br/>
5 Support Vector Machine | Lecture 8 Kernel<br/>
<br/>
5 Support Vector Machine | Lecture 9 SVM with Kernel<br/>
<br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/032">032. Week 06. Training, Testing, Regularization - 01. Overfitting, Underfitting</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/basic/033">033. Week 06. Training, Testing, Regularization - 02. Trade-off relation between bias and variance</a><br/>
<br/>
6 Training Testing and Regularization | Lecture 3 Occamâ€™s razor<br/>
<br/>
6 Training Testing and Regularization | Lecture 4 Cross Validation<br/>
<br/>
6 Training Testing and Regularization | Lecture 5 Performance Metrics<br/>
<br/>
6 Training Testing and Regularization | Lecture 6 Regularization<br/>
<br/>
6 Training Testing and Regularization | Lecture 7 Regularization Approximation<br/>
<br/>
7 Bayesian Network | Lecture 1 Probability Concepts<br/>
<br/>
7 Bayesian Network | Lecture 2 Probability Theorems<br/>
<br/>
7 Bayesian Network | Lecture 3 Interpretation of Bayesian Network<br/>
<br/>
7 Bayesian Network | Lecture 4 Bayes Ball Algorithm<br/>
<br/>
7 Bayesian Network | Lecture 5 Factorization of Bayesian networks<br/>
<br/>
7 Bayesian Network | Lecture 6 Inference Question on B. Networks<br/>
<br/>
7 Bayesian Network | Lecture 7 Variable Elimination<br/>
<br/>
7 Bayesian Network | Lecture 8 Potential Function and Clique Graph<br/>
<br/>
7 Bayesian Network | Lecture 9 Potential Function and Clique Graph<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 1 K-Means 1<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 2 K-Means 2<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 3 Multi<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 4 Multivar.<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 5 G.M.M<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 6 EM(Elimination-Maximization) step<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 7 Relation<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 8 EM(Elimination-Maximization)<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 9 Deriv. EM<br/>
<br/>
9 Hidden Markov Model | Lecture 1 Concept of Hidden Markov Model<br/>
<br/>
9 Hidden Markov Model | Lecture 2 Joint, Marginal Probability of HMM<br/>
<br/>
9 Hidden Markov Model | Lecture 3 Forward-Backward Probability Calculation<br/>
<br/>
9 Hidden Markov Model | Lecture 4 Viterbi Decoding Algorithm<br/>
<br/>
9 Hidden Markov Model | Lecture 5 Baum-Welch Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 1 Forward Sampling<br/>
<a href='https://youngminpark2559.github.io/mltheory/mnic/basic/010_Sampling_Based_Inference/002_Rejection_Sampling.html'>002_Rejection_Sampling</a><br/>
10 Sampling Based Inference | Lecture 3 Importance Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 4 Markov Chain<br/>
<br/>
10 Sampling Based Inference | Lecture 5 Markov Chain for Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 6 Metropolis-Hastings Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 7 Gibbs Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 8 Understand the LDA(Latent Dirichlet Allocation)<br/>
<br/>
10 Sampling Based Inference | Lecture 9 Gibbs Sampling for LDA (1)<br/>
<br/>
10 Sampling Based Inference | Lecture 10 Gibbs Sampling for LDA (2)<br/>

<br/><br/>
================================================================================<br/>
Todo<br/>
<h1>ICMoon - Machine Learning Advanced</h1>
https://www.youtube.com/playlist?list=PLbhbGI_ppZIRPeAjprW9u9A46IJlGFdLn<br/><br/>

1 Variational Inference | Lecture 1<br/>
<br/>
1 Variational Inference | Lecture 2<br/>
<br/>
1 Variational Inference | Lecture 3<br/>
<br/>
1 Variational Inference | Lecture 4<br/>
<br/>
1 Variational Inference | Lecture 5<br/>
<br/>
1 Variational Inference | Lecture 6<br/>
<br/>
1 Variational Inference | Lecture 7<br/>
<br/>
1 Variational Inference | Lecture 8<br/>
&nbsp&nbsp<a href="https://youngminpark2559.github.io/mltheory/advanced/001_Variational_Inference/009">009/main</a><br/>
<br/>
1 Variational Inference | Lecture 10<br/>
<br/>
1 Variational Inference | Lecture 11<br/>
<br/>
1 Variational Inference | Lecture 12<br/>
<br/>
1 Variational Inference | Lecture 13<br/>
<br/>
1 Variational Inference | Lecture 14<br/>
<br/>
1 Variational Inference | Lecture 15<br/>
<br/>
1 Variational Inference | Lecture 16<br/>
<br/>
1 Variational Inference | Lecture 17<br/>
<br/>
1 Variational Inference | Lecture 18<br/>
<br/>
1 Variational Inference | Lecture 19<br/>
<br/>
1 Variational Inference | Lecture 20<br/>
<br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/advanced/002_001">2 Dirhichlet Process | Lecture 1</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/advanced/002_002">2 Dirhichlet Process | Lecture 2</a><br/>
<a href="https://youngminpark2559.github.io/mltheory/mnic/advanced/002_003">2 Dirhichlet Process | Lecture 3</a><br/>
<br/>
2 Dirhichlet Process | Lecture 4<br/>
<br/>
2 Dirhichlet Process | Lecture 5<br/>
<br/>
2 Dirhichlet Process | Lecture 6<br/>
<br/>
2 Dirhichlet Process | Lecture 7<br/>
<br/>
2 Dirhichlet Process | Lecture 8<br/>
<br/>
2 Dirhichlet Process | Lecture 9<br/>
<br/>
2 Dirhichlet Process | Lecture 10<br/>
<br/>
2 Dirhichlet Process | Lecture 11<br/>
<br/>
2 Dirhichlet Process | Lecture 12<br/>
<br/>
2 Dirhichlet Process | Lecture 13<br/>
<br/>
2 Dirhichlet Process | Lecture 14<br/>
<br/>
2 Dirhichlet Process | Lecture 15<br/>
<br/>
2 Dirhichlet Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 1<br/>
<br/>
3 Gaussian Process | Lecture 2<br/>
<br/>
3 Gaussian Process | Lecture 3<br/>
<br/>
3 Gaussian Process | Lecture 4<br/>
<br/>
3 Gaussian Process | Lecture 5<br/>
<br/>
3 Gaussian Process | Lecture 6<br/>
<br/>
3 Gaussian Process | Lecture 7<br/>
<br/>
3 Gaussian Process | Lecture 8<br/>
<br/>
3 Gaussian Process | Lecture 9<br/>
<br/>
3 Gaussian Process | Lecture 10<br/>
<br/>
3 Gaussian Process | Lecture 11<br/>
<br/>
3 Gaussian Process | Lecture 12<br/>
<br/>
3 Gaussian Process | Lecture 13<br/>
<br/>
3 Gaussian Process | Lecture 14<br/>
<br/>
3 Gaussian Process | Lecture 15<br/>
<br/>
3 Gaussian Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 17<br/>
<br/>
3 Gaussian Process | Lecture 18<br/>
<br/>
3 Gaussian Process | Lecture 19<br/>
<br/>
3 Gaussian Process | Lecture 20<br/>
<br/>
3 Gaussian Process | Lecture 21<br/>
<br/>
4 Artificial Neural Network | Lecture 1<br/>
<br/>
4 Artificial Neural Network | Lecture 2<br/>
<br/>
4 Artificial Neural Network | Lecture 3<br/>
<br/>
4 Artificial Neural Network | Lecture 4<br/>
<br/>
4 Artificial Neural Network | Lecture 5<br/>
<br/>
4 Artificial Neural Network | Lecture 6<br/>
<br/>
4 Artificial Neural Network | Lecture 7<br/>
<br/>
4 Artificial Neural Network | Lecture 8<br/>

<br/><br/>
