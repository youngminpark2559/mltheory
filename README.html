<h2>Machine Learning</h2><br/><br/>

JJ - Outline concept of a pattern recognition<br/>
10-04 Nonparametric density estimation 
<a href="https://youngmtool.github.io/machinelearning/jjpr/10-04">https://youngmtool.github.io/machinelearning/jjpr/10-04</a><br/>
10-05 Nonparametric density estimation 
<a href="https://youngmtool.github.io/machinelearning/jjpr/10-05">https://youngmtool.github.io/machinelearning/jjpr/10-05</a><br/>
10-06 Nonparametric density estimation 
<a href="https://youngmtool.github.io/machinelearning/jjpr/10-06">https://youngmtool.github.io/machinelearning/jjpr/10-06</a><br/>
10-07 Nonparametric density estimation 
<a href="https://youngmtool.github.io/machinelearning/jjpr/10-07">https://youngmtool.github.io/machinelearning/jjpr/10-07</a><br/>
11-01 Clustring 
<a href="https://youngmtool.github.io/machinelearning/jjpr/11-01">https://youngmtool.github.io/machinelearning/jjpr/11-01</a><br/>
11-02 Clustring 
<a href="https://youngmtool.github.io/machinelearning/jjpr/11-02">https://youngmtool.github.io/machinelearning/jjpr/11-02</a><br/>
11-03 Clustring 
<a href="https://youngmtool.github.io/machinelearning/jjpr/11-03">https://youngmtool.github.io/machinelearning/jjpr/11-03</a><br/>
11-04 Clustring 
<a href="https://youngmtool.github.io/machinelearning/jjpr/11-04">https://youngmtool.github.io/machinelearning/jjpr/11-04</a><br/>
12-01 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/machinelearning/jjpr/12-01">https://youngmtool.github.io/machinelearning/jjpr/12-01</a><br/>
12-02 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/machinelearning/jjpr/12-02">https://youngmtool.github.io/machinelearning/jjpr/12-02</a><br/>
12-03 Short video 1 for the eigenvector and eigenvalue<br/>  
12-04 Short video 2 for the eigenvector and eigenvalue<br/>
12-05 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/machinelearning/jjpr/12-05">https://youngmtool.github.io/machinelearning/jjpr/12-05</a><br/>

<br/><br/>

TAcademy - Machine Learning Concepts<br/>
04. Decision Tree
<a href="https://youngmtool.github.io/machinelearning/TAcademy/04">https://youngmtool.github.io/machinelearning/TAcademy/04</a><br/>
05. Suppor Vector Machine(SVM)
<a href="https://youngmtool.github.io/machinelearning/TAcademy/05">https://youngmtool.github.io/machinelearning/TAcademy/05</a><br/>
10. Recurrent Neural Net(RNN) 
<a href="https://youngmtool.github.io/machinelearning/TAcademy/10">https://youngmtool.github.io/machinelearning/TAcademy/10</a><br/>
12. Example applications with an applied deep learning
<a href="https://youngmtool.github.io/machinelearning/TAcademy/12">https://youngmtool.github.io/machinelearning/TAcademy/12</a><br/>

<br/><br/>

CWLee - Deep learning concept<br/>
02. Regression and gradient descent
<a href="https://youngmtool.github.io/machinelearning/cwlee/02">https://youngmtool.github.io/machinelearning/cwlee/02</a><br/>
03. Gradient Descent & Normal Equation
<a href="https://youngmtool.github.io/machinelearning/cwlee/03">https://youngmtool.github.io/machinelearning/cwlee/03</a><br/>
04. Logistic Regression
<a href="https://youngmtool.github.io/machinelearning/cwlee/04">https://youngmtool.github.io/machinelearning/cwlee/04</a><br/>
05. Loss function in logistic regression
<a href="https://youngmtool.github.io/machinelearning/cwlee/05">https://youngmtool.github.io/machinelearning/cwlee/05</a><br/>
06. Implementing a neural net(NN)
<a href="https://youngmtool.github.io/machinelearning/cwlee/06">https://youngmtool.github.io/machinelearning/cwlee/06</a><br/>
07. Backpropagation in neural net
<a href="https://youngmtool.github.io/machinelearning/cwlee/07">https://youngmtool.github.io/machinelearning/cwlee/07</a><br/>
08. softmax function
<a href="https://youngmtool.github.io/machinelearning/cwlee/08">https://youngmtool.github.io/machinelearning/cwlee/08</a><br/>
09. Convolutional Neural Net(CNN)
<a href="https://youngmtool.github.io/machinelearning/cwlee/09">https://youngmtool.github.io/machinelearning/cwlee/09</a><br/>
10. CNN Back Propagation
<a href="https://youngmtool.github.io/machinelearning/cwlee/10">https://youngmtool.github.io/machinelearning/cwlee/10</a><br/>
11. Local Minima
<a href="https://youngmtool.github.io/machinelearning/cwlee/11">https://youngmtool.github.io/machinelearning/cwlee/11</a><br/>
12. Unsupervised Pre-training for CNN
<a href="https://youngmtool.github.io/machinelearning/cwlee/12">https://youngmtool.github.io/machinelearning/cwlee/12</a><br/>
13. RNN Introduction
<a href="https://youngmtool.github.io/machinelearning/cwlee/13">https://youngmtool.github.io/machinelearning/cwlee/13</a><br/>
14. Back Propagation in RNN
<a href="https://youngmtool.github.io/machinelearning/cwlee/14">https://youngmtool.github.io/machinelearning/cwlee/14</a><br/>

<br/><br/>

Todo<br/>
HWLee - Outline concept of a random process <br/>
01-01. Introduction. Probability axioms and random variables PDF/PMF and CDF  <br/>
01-01. Introduction. Probability axioms and random variables PDF/PMF and CDF  <br/>
02-01. Function of random variables Definitions of convergence Convergence in probability, convergence with probability 1, convergence in distribution  <br/>
02-01. Function of random variables Definitions of convergence Convergence in probability, convergence with probability 1, convergence in distribution  <br/>
03-01. Useful inequalities and law of large numbers. Central limit theorem Markov inequality, Chebyshev inequality, Chernoff bound  <br/>
03-01. Useful inequalities and law of large numbers. Central limit theorem Markov inequality, Chebyshev inequality, Chernoff bound  <br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes  <br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes  <br/>
05-01. Discrete-time Markov chains and steady-state behavior Definition, state transition probability, Markov property  <br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review  <br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review  <br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times  <br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times  <br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula  <br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula  <br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization  <br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization  <br/>
10-01. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models 
<a href="https://youngmtool.github.io/machinelearning/hwlee/10-01">https://youngmtool.github.io/machinelearning/hwlee/10-01</a><br/>
10-02. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models  <br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis  <br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis  <br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms  <br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms  <br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms  <br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms <br/>

<br/><br/>

Todo <br/>
ICMoon - Machine Learning Basic<br/>
1 Motivations and Basics | Lecture 1 Motivation<br/>
<br/>
1 Motivations and Basics | Lecture 2 MLE<br/>
<br/>
1 Motivations and Basics | Lecture 3 MAP<br/>
<br/>
1 Motivations and Basics | Lecture 4 Probability and Distribution<br/>
<br/>
2 Fundamentals of Machine Learning | Lecture 1 Rule-Based Machine Learning<br/>
<br/>
2 Fundamentals of Machine Learning | Lecture 2 Intro. to Rule-Based<br/>
<br/>
2 Fundamentals of Machine Learning | Lecture 3 Introduction to Decision Tree<br/>
<br/>
2 Fundamentals of Machine Learning | Lecture 4 Entropy and Infomation Gain<br/>
<br/>
2 Fundamentals of Machine Learning | Lecture 5 How to create a Decision Tree<br/>
<br/>
3 Naive Bayes Classifier | Lecture 1 Optimal Classification<br/>
<br/>
3 Naive Bayes Classifier | Lecture 2 Conditional Independence<br/>
<br/>
3 Naive Bayes Classifier | Lecture 3 Naive Bayes Classifier<br/>
<br/>
3 Naive Bayes Classifier | Lecture 4 Naive Bayes Classifier with Matlab<br/>
<br/>
4 Logistic Regression | Lecture 1 Decision Boundary<br/>
<br/>
4 Logistic Regression | Lecture 2 Introduction to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 3 Logistic Regression Parameter Approximation 1<br/>
<br/>
4 Logistic Regression | Lecture 4 Gradient method<br/>
<br/>
4 Logistic Regression | Lecture 5 How Gradient method works<br/>
<br/>
4 Logistic Regression | Lecture 6 Logistic Regression Parameter Approximation 2<br/>
<br/>
4 Logistic Regression | Lecture 7 Naive Bayes to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 8 Naive Bayes vs Logistic Regression<br/>
<br/>
5 Support Vector Machine | Lecture 1 Decision boundary with Margin<br/>
<br/>
5 Support Vector Machine | Lecture 2 Maximizing the Margin<br/>
<br/>
5 Support Vector Machine | Lecture 3 SVM with Matlab<br/>
<br/>
5 Support Vector Machine | Lecture 4 Error Handling in SVM<br/>
<br/>
5 Support Vector Machine | Lecture 5 Soft Margin with SVM<br/>
<br/>
5 Support Vector Machine | Lecture 6 Rethinking of SVM<br/>
<br/>
5 Support Vector Machine | Lecture 7 Primal, Dual with KKT Condition<br/>
<br/>
5 Support Vector Machine | Lecture 8 Kernel<br/>
<br/>
5 Support Vector Machine | Lecture 9 SVM with Kernel<br/>
<br/>
6 Training Testing and Regularization | Lecture 1 Over, Under fitting<br/>
<br/>
6 Training Testing and Regularization | Lecture 2 Bias and Variance<br/>
<br/>
6 Training Testing and Regularization | Lecture 3 Occam’s razor<br/>
<br/>
6 Training Testing and Regularization | Lecture 4 Cross Validation<br/>
<br/>
6 Training Testing and Regularization | Lecture 5 Performance Metrics<br/>
<br/>
6 Training Testing and Regularization | Lecture 6 Regularization<br/>
<br/>
6 Training Testing and Regularization | Lecture 7 Regularization Approximation<br/>
<br/>
7 Bayesian Network | Lecture 1 Probability Concepts<br/>
<br/>
7 Bayesian Network | Lecture 2 Probability Theorems<br/>
<br/>
7 Bayesian Network | Lecture 3 Interpretation of Bayesian Network<br/>
<br/>
7 Bayesian Network | Lecture 4 Bayes Ball Algorithm<br/>
<br/>
7 Bayesian Network | Lecture 5 Factorization of Bayesian networks<br/>
<br/>
7 Bayesian Network | Lecture 6 Inference Question on B. Networks<br/>
<br/>
7 Bayesian Network | Lecture 7 Variable Elimination<br/>
<br/>
7 Bayesian Network | Lecture 8 Potential Function and Clique Graph<br/>
<br/>
7 Bayesian Network | Lecture 9 Potential Function and Clique Graph<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 1 K-Means 1<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 2 K-Means 2<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 3 Multi<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 4 Multivar.<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 5 G.M.M<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 6 EM(Elimination-Maximization) step<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 7 Relation<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 8 EM(Elimination-Maximization)<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 9 Deriv. EM<br/>
<br/>
9 Hidden Markov Model | Lecture 1 Concept of Hidden Markov Model<br/>
<br/>
9 Hidden Markov Model | Lecture 2 Joint, Marginal Probability of HMM<br/>
<br/>
9 Hidden Markov Model | Lecture 3 Forward-Backward Probability Calculation<br/>
<br/>
9 Hidden Markov Model | Lecture 4 Viterbi Decoding Algorithm<br/>
<br/>
9 Hidden Markov Model | Lecture 5 Baum-Welch Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 1 Forward Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 2 Rejection Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 3 Importance Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 4 Markov Chain<br/>
<br/>
10 Sampling Based Inference | Lecture 5 Markov Chain for Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 6 Metropolis-Hastings Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 7 Gibbs Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 8 Understand the LDA(Latent Dirichlet Allocation)<br/>
<br/>
10 Sampling Based Inference | Lecture 9 Gibbs Sampling for LDA (1)<br/>
<br/>
10 Sampling Based Inference | Lecture 10 Gibbs Sampling for LDA (2)<br/>

<br/><br/>
Todo<br/>
ICMoon - Machine Learning Advanced<br/>
1 Variational Inference | Lecture 1<br/>
<br/>
1 Variational Inference | Lecture 2<br/>
<br/>
1 Variational Inference | Lecture 3<br/>
<br/>
1 Variational Inference | Lecture 4<br/>
<br/>
1 Variational Inference | Lecture 5<br/>
<br/>
1 Variational Inference | Lecture 6<br/>
<br/>
1 Variational Inference | Lecture 7<br/>
<br/>
1 Variational Inference | Lecture 8<br/>
<br/>
1 Variational Inference | Lecture 9<br/>
<br/>
1 Variational Inference | Lecture 10<br/>
<br/>
1 Variational Inference | Lecture 11<br/>
<br/>
1 Variational Inference | Lecture 12<br/>
<br/>
1 Variational Inference | Lecture 13<br/>
<br/>
1 Variational Inference | Lecture 14<br/>
<br/>
1 Variational Inference | Lecture 15<br/>
<br/>
1 Variational Inference | Lecture 16<br/>
<br/>
1 Variational Inference | Lecture 17<br/>
<br/>
1 Variational Inference | Lecture 18<br/>
<br/>
1 Variational Inference | Lecture 19<br/>
<br/>
1 Variational Inference | Lecture 20<br/>
<br/>
2 Dirhichlet Process | Lecture 1<br/>
<br/>
2 Dirhichlet Process | Lecture 2<br/>
<br/>
2 Dirhichlet Process | Lecture 3<br/>
<br/>
2 Dirhichlet Process | Lecture 4<br/>
<br/>
2 Dirhichlet Process | Lecture 5<br/>
<br/>
2 Dirhichlet Process | Lecture 6<br/>
<br/>
2 Dirhichlet Process | Lecture 7<br/>
<br/>
2 Dirhichlet Process | Lecture 8<br/>
<br/>
2 Dirhichlet Process | Lecture 9<br/>
<br/>
2 Dirhichlet Process | Lecture 10<br/>
<br/>
2 Dirhichlet Process | Lecture 11<br/>
<br/>
2 Dirhichlet Process | Lecture 12<br/>
<br/>
2 Dirhichlet Process | Lecture 13<br/>
<br/>
2 Dirhichlet Process | Lecture 14<br/>
<br/>
2 Dirhichlet Process | Lecture 15<br/>
<br/>
2 Dirhichlet Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 1<br/>
<br/>
3 Gaussian Process | Lecture 2<br/>
<br/>
3 Gaussian Process | Lecture 3<br/>
<br/>
3 Gaussian Process | Lecture 4<br/>
<br/>
3 Gaussian Process | Lecture 5<br/>
<br/>
3 Gaussian Process | Lecture 6<br/>
<br/>
3 Gaussian Process | Lecture 7<br/>
<br/>
3 Gaussian Process | Lecture 8<br/>
<br/>
3 Gaussian Process | Lecture 9<br/>
<br/>
3 Gaussian Process | Lecture 10<br/>
<br/>
3 Gaussian Process | Lecture 11<br/>
<br/>
3 Gaussian Process | Lecture 12<br/>
<br/>
3 Gaussian Process | Lecture 13<br/>
<br/>
3 Gaussian Process | Lecture 14<br/>
<br/>
3 Gaussian Process | Lecture 15<br/>
<br/>
3 Gaussian Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 17<br/>
<br/>
3 Gaussian Process | Lecture 18<br/>
<br/>
3 Gaussian Process | Lecture 19<br/>
<br/>
3 Gaussian Process | Lecture 20<br/>
<br/>
3 Gaussian Process | Lecture 21<br/>
<br/>
4 Artificial Neural Network | Lecture 1<br/>
<br/>
4 Artificial Neural Network | Lecture 2<br/>
<br/>
4 Artificial Neural Network | Lecture 3<br/>
<br/>
4 Artificial Neural Network | Lecture 4<br/>
<br/>
4 Artificial Neural Network | Lecture 5<br/>
<br/>
4 Artificial Neural Network | Lecture 6<br/>
<br/>
4 Artificial Neural Network | Lecture 7<br/>
<br/>
4 Artificial Neural Network | Lecture 8<br/>

<br/><br/>


