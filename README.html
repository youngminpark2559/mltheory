<h2>Machine Learning Theory</h2>

JJ - Outline concept of a pattern recognition<br/>
01_02 process of pattern recognition 
<a href="https://youngmtool.github.io/mltheory/jjpr/01_02.html">01_02</a><br/>
01_03 feature, feature vector, feature space, pattern, scatter plot 
<a href="https://youngmtool.github.io/mltheory/jjpr/01_03.html">01_03</a><br/>
02_001
<a href="https://youngmtool.github.io/mltheory/jjpr/02_001.html">02_001</a><br/>
02_002
<a href="https://youngmtool.github.io/mltheory/jjpr/02_002.html">02_002</a><br/>
02_003
<a href="https://youngmtool.github.io/mltheory/jjpr/02_003.html">02_003</a><br/>
02_004
<a href="https://youngmtool.github.io/mltheory/jjpr/02_004.html">02_004</a><br/>
02_005
<a href="https://youngmtool.github.io/mltheory/jjpr/02_005.html">02_005</a><br/>
02_006
<a href="https://youngmtool.github.io/mltheory/jjpr/02_006.html">02_006</a><br/>
03_001
<a href="https://youngmtool.github.io/mltheory/jjpr/03_001.html">03_001</a><br/>
03_002
<a href="https://youngmtool.github.io/mltheory/jjpr/03_002.html">03_002</a><br/>
03_003
<a href="https://youngmtool.github.io/mltheory/jjpr/03_003.html">03_003</a><br/>
03_004
<a href="https://youngmtool.github.io/mltheory/jjpr/03_004.html">03_004</a><br/>
03_005
<a href="https://youngmtool.github.io/mltheory/jjpr/03_005.html">03_005</a><br/>
10-04 Nonparametric density estimation 
<a href="https://youngmtool.github.io/mltheory/jjpr/10-04">https://youngmtool.github.io/mltheory/jjpr/10-04</a><br/>
10-05 Nonparametric density estimation 
<a href="https://youngmtool.github.io/mltheory/jjpr/10-05">https://youngmtool.github.io/mltheory/jjpr/10-05</a><br/>
10-06 Nonparametric density estimation 
<a href="https://youngmtool.github.io/mltheory/jjpr/10-06">https://youngmtool.github.io/mltheory/jjpr/10-06</a><br/>
10-07 Nonparametric density estimation 
<a href="https://youngmtool.github.io/mltheory/jjpr/10-07">https://youngmtool.github.io/mltheory/jjpr/10-07</a><br/>
11-01 Clustring 
<a href="https://youngmtool.github.io/mltheory/jjpr/11-01">https://youngmtool.github.io/mltheory/jjpr/11-01</a><br/>
11-02 Clustring 
<a href="https://youngmtool.github.io/mltheory/jjpr/11-02">https://youngmtool.github.io/mltheory/jjpr/11-02</a><br/>
11-03 Clustring 
<a href="https://youngmtool.github.io/mltheory/jjpr/11-03">https://youngmtool.github.io/mltheory/jjpr/11-03</a><br/>
11-04 Clustring 
<a href="https://youngmtool.github.io/mltheory/jjpr/11-04">https://youngmtool.github.io/mltheory/jjpr/11-04</a><br/>
12-01 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/12-01">https://youngmtool.github.io/mltheory/jjpr/12-01</a><br/>
12-02 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/12-02">https://youngmtool.github.io/mltheory/jjpr/12-02</a><br/>
12-03 Short video 1 for the eigenvector and eigenvalue<br/>  
12-04 Short video 2 for the eigenvector and eigenvalue<br/>
12-05 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/12-05">https://youngmtool.github.io/mltheory/jjpr/12-05</a><br/>
12-06 Dimentionality reduction by PCA(principle component analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/12-06">https://youngmtool.github.io/mltheory/jjpr/12-06</a><br/>
13-01 LDA(linear discriminant analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/13-01">https://youngmtool.github.io/mltheory/jjpr/13-01</a><br/>
13-02 LDA(linear discriminant analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/13-02">https://youngmtool.github.io/mltheory/jjpr/13-02</a><br/>
13-03 LDA(linear discriminant analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/13-03">https://youngmtool.github.io/mltheory/jjpr/13-03</a><br/>
13-04 LDA(linear discriminant analysis) 
<a href="https://youngmtool.github.io/mltheory/jjpr/13-04">https://youngmtool.github.io/mltheory/jjpr/13-04</a><br/>

<br/><br/>

TAcademy - Machine Learning Concepts<br/>
04. Decision Tree
<a href="https://youngmtool.github.io/mltheory/TAcademy/04">https://youngmtool.github.io/mltheory/TAcademy/04</a><br/>
05. Suppor Vector Machine(SVM)
<a href="https://youngmtool.github.io/mltheory/TAcademy/05">https://youngmtool.github.io/mltheory/TAcademy/05</a><br/>
10. Recurrent Neural Net(RNN) 
<a href="https://youngmtool.github.io/mltheory/TAcademy/10">https://youngmtool.github.io/mltheory/TAcademy/10</a><br/>
12. Example applications with an applied deep learning
<a href="https://youngmtool.github.io/mltheory/TAcademy/12">https://youngmtool.github.io/mltheory/TAcademy/12</a><br/>

<br/><br/>

CWLee - Deep learning concept<br/>
02. Regression and gradient descent
<a href="https://youngmtool.github.io/mltheory/cwlee/02">https://youngmtool.github.io/mltheory/cwlee/02</a><br/>
03. Gradient Descent & Normal Equation 
<a href="https://youngmtool.github.io/mltheory/cwlee/03">https://youngmtool.github.io/mltheory/cwlee/03</a><br/>
04. Logistic Regression 
<a href="https://youngmtool.github.io/mltheory/cwlee/04">https://youngmtool.github.io/mltheory/cwlee/04</a><br/>
05. Loss function in logistic regression 
<a href="https://youngmtool.github.io/mltheory/cwlee/05">https://youngmtool.github.io/mltheory/cwlee/05</a><br/>
06. Implementing a neural net(NN) 
<a href="https://youngmtool.github.io/mltheory/cwlee/06">https://youngmtool.github.io/mltheory/cwlee/06</a><br/>
07. Backpropagation in neural net 
<a href="https://youngmtool.github.io/mltheory/cwlee/07">https://youngmtool.github.io/mltheory/cwlee/07</a><br/>
08. softmax function 
<a href="https://youngmtool.github.io/mltheory/cwlee/08">https://youngmtool.github.io/mltheory/cwlee/08</a><br/>
09. Convolutional Neural Net(CNN) 
<a href="https://youngmtool.github.io/mltheory/cwlee/09">https://youngmtool.github.io/mltheory/cwlee/09</a><br/>
10. CNN Back Propagation 
<a href="https://youngmtool.github.io/mltheory/cwlee/10">https://youngmtool.github.io/mltheory/cwlee/10</a><br/>
11. Local Minima 
<a href="https://youngmtool.github.io/mltheory/cwlee/11">https://youngmtool.github.io/mltheory/cwlee/11</a><br/>
12. Unsupervised Pre-training for CNN 
<a href="https://youngmtool.github.io/mltheory/cwlee/12">https://youngmtool.github.io/mltheory/cwlee/12</a><br/>
13. RNN Introduction 
<a href="https://youngmtool.github.io/mltheory/cwlee/13">https://youngmtool.github.io/mltheory/cwlee/13</a><br/>
14. Back Propagation in RNN 
<a href="https://youngmtool.github.io/mltheory/cwlee/14">https://youngmtool.github.io/mltheory/cwlee/14</a><br/>

<br/><br/>

HWLee - Outline concept of a random process <br/>
01-01. Probability axioms and random variables 
<a href="https://youngmtool.github.io/mltheory/hwlee/01-01">https://youngmtool.github.io/mltheory/hwlee/01-01</a><br/>
01-02. Probability axioms and random variables 
<a href="https://youngmtool.github.io/mltheory/hwlee/01-02">https://youngmtool.github.io/mltheory/hwlee/01-02</a><br/>
02-01. Function of random variables, Definitions of convergence, Convergence in probability, Convergence with probability 1, Convergence in distribution 
<a href="https://youngmtool.github.io/mltheory/hwlee/02-01">https://youngmtool.github.io/mltheory/hwlee/02-01</a><br/>
02-02. Function of random variables, Definitions of convergence, Convergence in probability, Convergence with probability 1, Convergence in distribution 
<a href="https://youngmtool.github.io/mltheory/hwlee/02-02">https://youngmtool.github.io/mltheory/hwlee/02-02</a><br/>
03-01. Useful inequalities and law of large numbers. Central limit theorem. Markov inequality. Chebyshev inequality. Chernoff bound. 
<a href="https://youngmtool.github.io/mltheory/hwlee/03-01">https://youngmtool.github.io/mltheory/hwlee/03-01</a><br/>
03-01. Useful inequalities and law of large numbers. Central limit theorem. Markov inequality. Chebyshev inequality. Chernoff bound. 
<a href="https://youngmtool.github.io/mltheory/hwlee/03-02">https://youngmtool.github.io/mltheory/hwlee/03-02</a><br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes  <br/>
04-01. Bernoulli process and Poisson process Definitions and properties of Bernoulli and Poisson processes  <br/>
05-01. Discrete-time Markov chains and steady-state behavior Definition, state transition probability, Markov property  <br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review  <br/>
06-01. Mixing time and midterm review Role of second largest eigenvalues and midterm review  <br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times  <br/>
07-01. M/M/1 queues Poisson arrival and exponential service, analysis of waiting times  <br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula  <br/>
08-01. M/G/1 queues and Pollaczek- Khinchin formula Definition of M/G/1 queue and derivation of Pollaczek-Khinchin formula  <br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization  <br/>
09-01. Estimation theory and Expectation- Maximization (EM) algorithm Bayesian estimation, expectation maximization  <br/>
10-01. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models 
<a href="https://youngmtool.github.io/mltheory/hwlee/10-01">https://youngmtool.github.io/mltheory/hwlee/10-01</a><br/>
10-02. Hidden Markov models (HMM) Modeling uncertain pheonomena using hidden Markov models  <br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis  <br/>
11-01. Counting processes and Renewal processes Definition of counting and renewal processes, and analysis  <br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms  <br/>
12-01. Randomized algorithms Applications of probability and stochastic processes to randomized algorithms  <br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms  <br/>
13-01. Randomized algorithms and course review Applications of probability and stochastic processes to randomized algorithms <br/>

<br/><br/>

dohkim - Data Science<br/>
008-001. introduction to pandas 
<a href="https://youngmtool.github.io/mltheory/dohkim/008-001">https://youngmtool.github.io/mltheory/dohkim/008-001</a><br/>
008-002. ID data, load csv, create csv, export csv 
<a href="https://youngmtool.github.io/mltheory/dohkim/008-002">https://youngmtool.github.io/mltheory/dohkim/008-002</a><br/>
008-003. dataframe indexer, loc[], iloc[], at[], iat[] 
<a href="https://youngmtool.github.io/mltheory/dohkim/008-003">https://youngmtool.github.io/mltheory/dohkim/008-003</a><br/>
008-004. dataframe manipulating data 
<a href="https://youngmtool.github.io/mltheory/dohkim/008-003">https://youngmtool.github.io/mltheory/dohkim/008-003</a><br/>
010-001. nltk package for natural language processing
<a href="https://youngmtool.github.io/mltheory/dohkim/010-001">https://youngmtool.github.io/mltheory/dohkim/010-001</a><br/>
025-001. bernoulli distribution
<a href="https://youngmtool.github.io/mltheory/dohkim/025-001">https://youngmtool.github.io/mltheory/dohkim/025-001</a><br/>
031-001. meaning of test and parameter estimation 
<a href="https://youngmtool.github.io/mltheory/dohkim/031-001">https://youngmtool.github.io/mltheory/dohkim/031-001</a><br/>
031-002. testing and p-value 
<a href="https://youngmtool.github.io/mltheory/dohkim/031-002">https://youngmtool.github.io/mltheory/dohkim/031-002</a><br/>
045-001. K-Means clustring 
<a href="https://youngmtool.github.io/mltheory/dohkim/045-001">https://youngmtool.github.io/mltheory/dohkim/045-001</a><br/>
048-002. naive bayes classification model 
<a href="https://youngmtool.github.io/mltheory/dohkim/048-002">https://youngmtool.github.io/mltheory/dohkim/048-002</a><br/>

<br/><br/>

icmoon - Machine Learning Basic<br/>
001. Week 01. Motivations and Basics - 01. Motivation 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/001">https://youngmtool.github.io/mltheory/mnic/basic/001</a><br/>
002. Week 01. Motivations and Basics - 02. MLE(maximum likelihood estimation) 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/002">https://youngmtool.github.io/mltheory/mnic/basic/002</a><br/>
003. Week 01. Motivations and Basics - 03. MAP(maximum posteriori estimation) 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/003">https://youngmtool.github.io/mltheory/mnic/basic/003</a><br/>
004. Week 01. Motivations and Basics - 04. Probability and Distribution 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/004">https://youngmtool.github.io/mltheory/mnic/basic/004</a><br/>
005. Week 02. Fundamentals of Machine Learning - 01. Rule-Based machine learning 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/005">https://youngmtool.github.io/mltheory/mnic/basic/005</a><br/>
008. Week 02. Fundamentals of Machine Learning - 04. Entropy and Info. Gain 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/008">https://youngmtool.github.io/mltheory/mnic/basic/008</a><br/>
009. Week 02. Fundamentals of Machine Learning - 05. How to create a Disicion tree 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/009">https://youngmtool.github.io/mltheory/mnic/basic/009</a><br/>
010. Week 03. Naive Bayes Classifier - 01. Optimal Classification 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/010">https://youngmtool.github.io/mltheory/mnic/basic/010</a><br/>
011. Week 03. Naive Bayes Classifier - 02. Conditional Independence 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/011">https://youngmtool.github.io/mltheory/mnic/basic/011</a><br/>
018. Week 04. Logistic Regression - 05. How Gradient method works 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/018">https://youngmtool.github.io/mltheory/mnic/basic/018</a><br/>
061. Week 10. Sampling Based Inference - 01. Forward Sampling 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/061">https://youngmtool.github.io/mltheory/mnic/basic/061</a><br/>

2 Fundamentals of Machine Learning | Lecture 2 Intro. to Rule-Based<br/>
2 Fundamentals of Machine Learning | Lecture 3 Introduction to Decision Tree<br/>
3 Naive Bayes Classifier | Lecture 3 Naive Bayes Classifier<br/>
<br/>
3 Naive Bayes Classifier | Lecture 4 Naive Bayes Classifier with Matlab<br/>
<br/>
4 Logistic Regression | Lecture 1 Decision Boundary<br/>
<br/>
4 Logistic Regression | Lecture 2 Introduction to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 3 Logistic Regression Parameter Approximation 1<br/>
<br/>
4 Logistic Regression | Lecture 4 Gradient method<br/>
<br/>
4 Logistic Regression | Lecture 5 How Gradient method works<br/>
<br/>
4 Logistic Regression | Lecture 6 Logistic Regression Parameter Approximation 2<br/>
<br/>
4 Logistic Regression | Lecture 7 Naive Bayes to Logistic Regression<br/>
<br/>
4 Logistic Regression | Lecture 8 Naive Bayes vs Logistic Regression<br/>
<br/>
5 Support Vector Machine | Lecture 1 Decision boundary with Margin<br/>
<br/>
5 Support Vector Machine | Lecture 2 Maximizing the Margin<br/>
<br/>
5 Support Vector Machine | Lecture 3 SVM with Matlab<br/>
<br/>
5 Support Vector Machine | Lecture 4 Error Handling in SVM<br/>
<br/>
5 Support Vector Machine | Lecture 5 Soft Margin with SVM<br/>
<br/>
5 Support Vector Machine | Lecture 6 Rethinking of SVM<br/>
<br/>
5 Support Vector Machine | Lecture 7 Primal, Dual with KKT Condition<br/>
<br/>
5 Support Vector Machine | Lecture 8 Kernel<br/>
<br/>
5 Support Vector Machine | Lecture 9 SVM with Kernel<br/>
<br/>
032. Week 06. Training, Testing, Regularization - 01. Overfitting, Underfitting 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/032">https://youngmtool.github.io/mltheory/mnic/basic/032</a><br/>
033. Week 06. Training, Testing, Regularization - 02. Trade-off relation between bias and variance 
<a href="https://youngmtool.github.io/mltheory/mnic/basic/033">https://youngmtool.github.io/mltheory/mnic/basic/033</a><br/>
<br/>
6 Training Testing and Regularization | Lecture 3 Occam’s razor<br/>
<br/>
6 Training Testing and Regularization | Lecture 4 Cross Validation<br/>
<br/>
6 Training Testing and Regularization | Lecture 5 Performance Metrics<br/>
<br/>
6 Training Testing and Regularization | Lecture 6 Regularization<br/>
<br/>
6 Training Testing and Regularization | Lecture 7 Regularization Approximation<br/>
<br/>
7 Bayesian Network | Lecture 1 Probability Concepts<br/>
<br/>
7 Bayesian Network | Lecture 2 Probability Theorems<br/>
<br/>
7 Bayesian Network | Lecture 3 Interpretation of Bayesian Network<br/>
<br/>
7 Bayesian Network | Lecture 4 Bayes Ball Algorithm<br/>
<br/>
7 Bayesian Network | Lecture 5 Factorization of Bayesian networks<br/>
<br/>
7 Bayesian Network | Lecture 6 Inference Question on B. Networks<br/>
<br/>
7 Bayesian Network | Lecture 7 Variable Elimination<br/>
<br/>
7 Bayesian Network | Lecture 8 Potential Function and Clique Graph<br/>
<br/>
7 Bayesian Network | Lecture 9 Potential Function and Clique Graph<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 1 K-Means 1<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 2 K-Means 2<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 3 Multi<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 4 Multivar.<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 5 G.M.M<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 6 EM(Elimination-Maximization) step<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 7 Relation<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 8 EM(Elimination-Maximization)<br/>
<br/>
8 K-Means Clustering and Gaussian Mixture Model | Lecture 9 Deriv. EM<br/>
<br/>
9 Hidden Markov Model | Lecture 1 Concept of Hidden Markov Model<br/>
<br/>
9 Hidden Markov Model | Lecture 2 Joint, Marginal Probability of HMM<br/>
<br/>
9 Hidden Markov Model | Lecture 3 Forward-Backward Probability Calculation<br/>
<br/>
9 Hidden Markov Model | Lecture 4 Viterbi Decoding Algorithm<br/>
<br/>
9 Hidden Markov Model | Lecture 5 Baum-Welch Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 1 Forward Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 2 Rejection Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 3 Importance Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 4 Markov Chain<br/>
<br/>
10 Sampling Based Inference | Lecture 5 Markov Chain for Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 6 Metropolis-Hastings Algorithm<br/>
<br/>
10 Sampling Based Inference | Lecture 7 Gibbs Sampling<br/>
<br/>
10 Sampling Based Inference | Lecture 8 Understand the LDA(Latent Dirichlet Allocation)<br/>
<br/>
10 Sampling Based Inference | Lecture 9 Gibbs Sampling for LDA (1)<br/>
<br/>
10 Sampling Based Inference | Lecture 10 Gibbs Sampling for LDA (2)<br/>

<br/><br/>
Todo<br/>
ICMoon - Machine Learning Advanced<br/>
1 Variational Inference | Lecture 1<br/>
<br/>
1 Variational Inference | Lecture 2<br/>
<br/>
1 Variational Inference | Lecture 3<br/>
<br/>
1 Variational Inference | Lecture 4<br/>
<br/>
1 Variational Inference | Lecture 5<br/>
<br/>
1 Variational Inference | Lecture 6<br/>
<br/>
1 Variational Inference | Lecture 7<br/>
<br/>
1 Variational Inference | Lecture 8<br/>
<br/>
1 Variational Inference | Lecture 9<br/>
<br/>
1 Variational Inference | Lecture 10<br/>
<br/>
1 Variational Inference | Lecture 11<br/>
<br/>
1 Variational Inference | Lecture 12<br/>
<br/>
1 Variational Inference | Lecture 13<br/>
<br/>
1 Variational Inference | Lecture 14<br/>
<br/>
1 Variational Inference | Lecture 15<br/>
<br/>
1 Variational Inference | Lecture 16<br/>
<br/>
1 Variational Inference | Lecture 17<br/>
<br/>
1 Variational Inference | Lecture 18<br/>
<br/>
1 Variational Inference | Lecture 19<br/>
<br/>
1 Variational Inference | Lecture 20<br/>
<br/>
2 Dirhichlet Process | Lecture 1<br/>
<br/>
2 Dirhichlet Process | Lecture 2<br/>
<br/>
2 Dirhichlet Process | Lecture 3<br/>
<br/>
2 Dirhichlet Process | Lecture 4<br/>
<br/>
2 Dirhichlet Process | Lecture 5<br/>
<br/>
2 Dirhichlet Process | Lecture 6<br/>
<br/>
2 Dirhichlet Process | Lecture 7<br/>
<br/>
2 Dirhichlet Process | Lecture 8<br/>
<br/>
2 Dirhichlet Process | Lecture 9<br/>
<br/>
2 Dirhichlet Process | Lecture 10<br/>
<br/>
2 Dirhichlet Process | Lecture 11<br/>
<br/>
2 Dirhichlet Process | Lecture 12<br/>
<br/>
2 Dirhichlet Process | Lecture 13<br/>
<br/>
2 Dirhichlet Process | Lecture 14<br/>
<br/>
2 Dirhichlet Process | Lecture 15<br/>
<br/>
2 Dirhichlet Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 1<br/>
<br/>
3 Gaussian Process | Lecture 2<br/>
<br/>
3 Gaussian Process | Lecture 3<br/>
<br/>
3 Gaussian Process | Lecture 4<br/>
<br/>
3 Gaussian Process | Lecture 5<br/>
<br/>
3 Gaussian Process | Lecture 6<br/>
<br/>
3 Gaussian Process | Lecture 7<br/>
<br/>
3 Gaussian Process | Lecture 8<br/>
<br/>
3 Gaussian Process | Lecture 9<br/>
<br/>
3 Gaussian Process | Lecture 10<br/>
<br/>
3 Gaussian Process | Lecture 11<br/>
<br/>
3 Gaussian Process | Lecture 12<br/>
<br/>
3 Gaussian Process | Lecture 13<br/>
<br/>
3 Gaussian Process | Lecture 14<br/>
<br/>
3 Gaussian Process | Lecture 15<br/>
<br/>
3 Gaussian Process | Lecture 16<br/>
<br/>
3 Gaussian Process | Lecture 17<br/>
<br/>
3 Gaussian Process | Lecture 18<br/>
<br/>
3 Gaussian Process | Lecture 19<br/>
<br/>
3 Gaussian Process | Lecture 20<br/>
<br/>
3 Gaussian Process | Lecture 21<br/>
<br/>
4 Artificial Neural Network | Lecture 1<br/>
<br/>
4 Artificial Neural Network | Lecture 2<br/>
<br/>
4 Artificial Neural Network | Lecture 3<br/>
<br/>
4 Artificial Neural Network | Lecture 4<br/>
<br/>
4 Artificial Neural Network | Lecture 5<br/>
<br/>
4 Artificial Neural Network | Lecture 6<br/>
<br/>
4 Artificial Neural Network | Lecture 7<br/>
<br/>
4 Artificial Neural Network | Lecture 8<br/>

<br/><br/>
