<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
img {
 width:500px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
13-02 LDA(linear discriminant analysis)
<xmp>
=========================================================
Projection by linear transform:
$$$y=W^{T}x$$$
W is (D,1)
Row vectors of W have direction for projection

x is (D,1)

=========================================================
2018-06-07 10-50-44.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 10-50-44.png"><xmp>

One of parameters which each class has is average parameter

Red ellipse : $$$\omega_{1}$$$ class data's distribution
Blue ellipse : $$$\omega_{2}$$$ class data's distribution

Average point of each class data's distribution is generally their centroids ($$$\mu_{1}, \mu_{1}$$$)

Average of class i $$$\mu_{i}$$$ :
$$$\mu_{i} = \frac{1}{N} \sum\limits_{x\in \omega_{i}} x$$$

=========================================================
How to find average after projection?

Average of class i $$$\mu_{i}$$$ :
$$$\mu_{i} = \frac{1}{N} \sum\limits_{x\in\omega_{i}}x$$$

$$$\tilde{\mu}_{i}$$$ : average of class i after projection

$$$\tilde{\mu}_{i} = \frac{1}{N}\sum\limits_{y\in \omega_{i}}y$$$
$$$\tilde{\mu}_{i} = \frac{1}{N}\sum\limits_{y\in \omega_{i}}W^{T}x$$$
$$$\tilde{\mu}_{i} = W^{T}\mu_{i}$$$

[Projection of $$$\mu_{i}$$$] == [projection of $$$\omega_{1}$$$ class data -> from projected data, find its average $$$\tilde{\mu}_{1}$$$]
2018-06-07 11-02-06.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 11-02-06.png"><xmp>

=========================================================
Case that you project data onto $$$x_{2}$$$ axis

2018-06-07 11-03-06.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 11-03-06.png"><xmp>

=========================================================
Distribution after projection onto each axix $$$x_{1}$$$ and $$$x_{2}$$$

2018-06-07 11-04-57.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 11-04-57.png"><xmp>

Case you project data onto $$$x_{1}$$$ :
Wide distance between averages $$$\mu_{1}$$$ and $$$\mu_{2}$$$
Red and blue are projected in wide range and they're overapped in some parts
This is good case of far distance between red class and blue class

Case you project data onto $$$x_{2}$$$ :
You get less overapped region
You get more clear separation of 2 classes
This is good case of class separation

But you should want both of them (wide range between averages and good separation)

=========================================================
First formular

You can choose distance between average points (centroids) of each used data 
as target function J(W) which you should maximize :
$$$J(W) = |\tilde{\mu_{1}} - \tilde{\mu_{2}}|$$$
$$$J(W) = |W^{T}\mu_{1}-W^{T}\mu_{2}|$$$
$$$J(W) = |W^{T}(\mu_{1}-\mu_{2})|$$$

J(W) : target function in respect to transform matrix W

2018-06-07 11-14-47.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 11-14-47.png"><xmp>

This target function J(W) finds most wide distance between 2 centroids
But this target function is insufficient, which will be complemented by following Fisher's LDA method

=========================================================
This method is suggested by Fisher

Target function J(W) which should be maximized :
$$$J(W) = \frac{|\tilde{\mu}_{1}-\tilde{\mu}_{2}|^{2}}{\tilde{S}_{1}^{2}+\tilde{S}_{2}^{2}}$$$

$$$\tilde{\mu}_{1}$$$ : average of class 1 data after projection
$$$\tilde{\mu}_{2}$$$ : average of class 2 data after projection
$$$|\tilde{\mu}_{1}-\tilde{\mu}_{2}|^{2}$$$ : square of $$$|\tilde{\mu}_{1}-\tilde{\mu}_{2}|$$$
$$$\tilde{S}_{1}^{2}$$$ : covariance matrix of class 1 data after projection
$$$\tilde{S}_{2}^{2}$$$ : covariance matrix of class 2 data after projection
$$$\tilde{S}_{1}^{2}+\tilde{S}_{2}^{2}$$$ : indicator related to within-class scatter (variance within class)

Fisher's LDA is linear fucntion $$$W^{T}x$$$ which maximizes above target function J(W)

To maximize J(W),
$$$|\tilde{\mu}_{1}-\tilde{\mu}_{2}|^{2}$$$ should be largest
$$$\tilde{S}_{1}^{2}+\tilde{S}_{2}^{2}$$$ should be lowest

=========================================================
Scatter (variance) of each class after projection $$$\tilde{S_{i}}$$$

$$$\tilde{S}_{i}^{2} = \sum\limits_{y\in\omega_{i}} (y-\tilde{\mu}_{i})^{2} $$$
$$$\tilde{S}_{i}^{2} = \sum\limits_{y\in\omega_{i}} (y-\tilde{\mu}_{i})(y-\tilde{\mu}_{i})^{T}$$$

=========================================================
Fisher's method finds transform matrix W which projects data in same class onto near region, at the same time, which makes far distances between centroids of classes

2018-06-07 13-08-09.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 13-08-09.png"><xmp>

Left axis :
short distance between centroids of projected class
good separation

below axis ($$$x_{1}$$$) :
long distance between centroids of projected class
bad separation (overapped)


=========================================================
How to find transform matrix W which maximizes target function J(W)?

That method is already found by mathematicians

Target function J(W) you've seen is this :
$$$J(W) = \frac{|\tilde{\mu}_{1}-\tilde{\mu}_{2}|^{2}}{\tilde{S}_{1}^{2}+\tilde{S}_{2}^{2}}$$$

To find optimal transform matrix W, you need to express target function J(W) in respect to W

Assumption : 
In multi-dimension feature space, scatter matrix S is identical form to covariance matrix only without scale factor $$$\frac{1}{N-1}$$$
Scatter matrix of class i $$$S_{i} = \sum\limits_{x\in\omega_{i}} (x-\mu_{i}) (x-\mu_{i})^{T}$$$
2018-06-07 13-19-48.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-07 13-19-48.png"><xmp>

Note
Covariance matrix $$$\Sigma= \frac{1}{N-1} \sum\limits_{x\in\omega_{i}} (x-\mu_{i}) (x-\mu_{i})^{T}$$$
 

Scatter matrix within class S_{with-in-class} :
$$$S_{with-in-class}^{2} = S_{1}^{2} + S_{2}^{2}$$$

=========================================================
After projection, scatter matrix within class \widetilde{S}_{with-in-class} has following relation
$$$\widetilde{S}_{1}^{2} + \widetilde{S}_{2}^{2} = W^{T}S_{within-class}W = \widetilde{S}_{within-class}$$$
$$$S_{within-class}$$$ : scatter matrix (covariance matrix) within class before projection
W : tranform matrix

Induce step :
$$$\widetilde{S}_{i}^{2} = \sum\limits_{y\in\omega_{i}} (y-\widetile{\mu}) (y-\widetile{\mu})^{T}$$$
$$$\widetilde{S}_{i}^{2} = \sum\limits_{y\in\omega_{i}} (W^{T}x-W^{T}\mu_{i}) (W^{T}x-W^{T}\mu_{i})^{T}$$$
$$$\widetilde{S}_{i}^{2} = \sum\limits_{y\in\omega_{i}} W^{T} (x-\mu_{i}) (x-\mu_{i})^{T} W$$$
$$$\widetilde{S}_{i}^{2} = W^{T}S_{i}^{2}W$$$
</xmp>
   </BODY>
</HTML>
