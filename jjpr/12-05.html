<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
img {
 width:900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
</HEAD>
<BODY>
12-05 Dimentionality reduction by PCA(principle component analysis)
<xmp>
@
Researchers was trying to induce way in linear algebra way, which satisfies following conditions :
keep variance (how much is data spread?) at high dimension as much as possible
reduce dimensionality

They were brought to find PCA methodology

@
2018-06-05 07-54-17.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-05 07-54-17.png"><xmp>
You can see variance plot as scatter plot of 2 dimensinal data onto 2 dimensional feature space
Each dot is one feature data consists of 2 features ($$$x_{1}, x_{2}$$$)
Intuitively, average wlll be located in center of entire data variance

2018-06-05 08-09-56.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-05 08-09-56.png"><xmp>
Amount of variance of this data can be represented by covariance matrix $$$\Sigma$$$

$$$\Sigma=\begin{bmatrix} \sigma_{1}^{2}& c_{12}\\c_{12}&\sigma_{1}^{2} \end{bmatrix}$$$.
$$$c_{12}$$$ : covariance of 1 and 2.

If you draw ellipse which reflects aspect of variance, that ellipse becomes covariance matirix of that data in geometry

@
What is relation between covariance matrix and PCA?

Covariance matrix is always squre matrix
All squre matrix has its "eigenvalue", "eigenvector" in respect to oneself

And you can express eigenvalue and eigenvector as components of covariance matrix $$$\Sigma$$$ in this way :
eigenvector column vector $$$u = \begin{matrix} u_{1}&u_{2} \end{matrix}$$$
eigenvalue matrix $$$\Lambda = \begin{matrix} \lambda_{1}&0 \\ 0&\lambda_{2} \end{matrix}$$$
eigenvector row vector $$$u^{T} = \begin{matrix} u_{1}\\u_{2} \end{matrix}$$$

$$$\Sigma = u*\Lambda * u^{T}$$$

Eigenvector of covariance matrix is identical to direction of long axis of ellipse
Eigenvvalue \lambda of covariance matrix is identical to length of long axis of ellipse


@
2018-06-05 08-16-17.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-05 08-16-17.png"><xmp>
When data has values of $$$u_{1}, u_{2}$$$, if you set $$$u_{2} =0$$$, only one component $$$u_{1}$$$ remains
In other words, only values projected onto $$$u_{1}$$$ axis remains

In similar way, let's reduce dimensionality and express $$$x_{1}, x_{2}$$$ onto $$$x_{1}x_{2}$$$ plane

@
If our goal is to keep aspect of variance of data,
2018-06-05 08-19-41.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-05 08-19-41.png"><xmp>
aspect of variance of data will be sustained if you project data onto long axis of ellipse $$$u_{1}$$$

2018-06-05 08-20-51.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-05 08-20-51.png"><xmp>
If you project data onto short axis of ellipse $$$u_{2}$$$, aspect of variance of data changes

In conclusion, if you want to keep aspect (variance) of feature vectors (in other words, goal of PCA), 
you should perform "dimensionality reduction" with long axis of ellipse (covariance matrix in geometry)

@
When you find direction of "principle components", which keeps variance of data,
first, you find covariance matrix of data,
second, you find eigenvalue and eigenvector from covariance matrix,
third, you find largest eigenvalue, which represents direction of principal components,
forth, you project data onto axis having direction of largest eigenvalue

=================================================================
Recap of PCA :
=================================================================
PCA is one of multivariate data processing ways,
which reduces dimensionality from data consists of high multidimensional feature vectors,
with sustaining characteristics of high multidimensional feature vectors

@
Feature vector has the number of data as much as the number of axis (dimensionality) of feature vector
For example, 3 dimensional feature data ($$$x_{1},x_{2},x_{3}$$$) can be expressed with 3 axixes (3 dimensional feature space)
That is, reducing dimensionality is identical to recuding axises

@
You need to find long axis corresponding to "principal components" of multivariate data in statistical method
Then, you can project data (feature vector) onto long axis, resulting in reduced dimensionality

@
Projection is inner product of matrices mathematically and geometrically
If you have feature vector and direction which you want to prject that feature vection with,
you can perform inner product between two

@
When you perform data analysis, you find "correlated coefficient"
there should be axis having largest "correlated coefficient" value

PCA 는 이러한 coefficient 가 높은 correlated 한 변량들의 변동 variance 를 줄이는 관점에서 correlated 가 없는 변량들로 이루어진 집합으로 기준 축을 변환한다. 그리고 이쪽 방향으로 feature vector 를 재배치한다.
=================================================================
Summary of PCA dimensionality reduction
=================================================================
Think of one vector $$$x \in R^{N}$$$
element of vector is set of R
Dimentionality of vector is N

Under condition $$$M<N$$$, 
you can approximate vector x into linear transform of M number of vectors (all vectors are independent) in following optimal way :
1. You find covariance matrix of x $$$\Sigma_{x}$$$
2. You find eigenvalue and eigenvector of this covariance matrix of x $$$\Sigma_{x}$$$
3. You find largest eigenvalue $$$\lambda_{i}$$$
4. You find eigenvector $$$\Phi_{i}$$$ corresponding to eigenvalue $$$\lambda_{i}$$$, which will be used as base vector
5. You project vector x onto eigenvector $$$\Phi_{i}$$$

Above way is to change N dimensional vector into M dimensional vector, without losing characteristics of N dimensianl vector x

@
Let's see how to compose transformation matrix W in mathematically PCA
PCA is actually task to find eigenvalue and eigenvector of covariance matrix
Composing transformation matrix W is task utilizing eigenvalue and eigenvector to create shape what you want

1. Let's suppose you have N number of sample
This sample has D dimensionality
Let's call data of this sample as $$$x_{n}$$$

You can find covariance matrix $$$\Sigma^{D\times D}$$$ of D dimension matrix

To find covariance matrix $$$\Sigma^{D\times D}$$$ of D dimension matrix,

1st, you need to find average vector $$$\mu$$$ from x_{n}
$$$\mu = \frac{1}{N} \sum\limits_{n=1}^{N} x_{n}$$$

2nd, you can find covariance matrix in this way
average[sum[(column)(row)]]
To find unbiased version, you divided sum by N-1
$$$\Sigma = \frac{1}{N-1} \sum\limits_{n=1}^{N} (x_{n}-\mu)(x_{n}-\mu)^{T}$$$

N : number of sample


2. You perform eigenvalue analsys to find eigenvalue and eigenvector
You don't need to manually find them but can use computer
In matlab, you perform eig(covariance-matrix),
then, you obtain matrix U (eigenvector matrix) and matrix $$$\Lambda$$$ (eigenvalue matrix)
$$$\Sigma^{D\times D} = U\Lambda U^{T} = \begin{bmatrix} u_{1}&...&u_{D} \end{bmatrix} \begin{bmatrix} \lambda_{1}&0&0\\0&...&0\\0&0&\lambda_{D} \end{bmatrix} \begin{bmatrix} u_{1}\\...\\u_{D} \end{bmatrix}$$$

3. Since covariance matrix \Sigma is $$$D\times D$$$, you obtain D number of eigenvalues
If you want to reduce data to M dimensionality, you choose M number of eigenvalues ($$$\lambda_{1}, ...,\lambda_{M}$$$) in descending order (choose eigenvalue from largest one), from D number of eigenvalues (\lambda_{1}, ...,\lambda_{D})
2018-06-05 09-26-17.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-05 09-26-17.png"><xmp>

4. You find eigenvectors $$$u_{1}, ..., u_{M}$$$ corresponding to chosen eigenvalues ($$$\lambda_{1}, ...,\lambda_{M}$$$)
You compose transformation matrix W by using eigenvectors $$$u_{1}, ..., u_{M}$$$
$$$W=\begin{bmatrix} u_{1}&...&u_{M} \end{bmatrix}$$$

5. If you want to project some feature vectors onto direction to reduce dimensionality,
you can multiply transformation matrix W by feature vector x :
$$$y=W^{T}x$$$
$$$y\in R^{M}$$$
$$$x\in R^{D}$$$
</xmp>
   </BODY>
</HTML>
