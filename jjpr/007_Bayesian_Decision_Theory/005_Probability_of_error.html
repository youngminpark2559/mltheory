<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 20px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 100px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
img {
 width:900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>

<xmp>
================================================================================
* Classifier can be considered as a tool which separate feature space into decision areas

* For example, separate feature height space.

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/jjpr/pic/2019_05_04_19:59:34.png' alt=''><xmp>

================================================================================
When you use Bayes classifier with 2 classes,
your goal is to separate feature space $$$R$$$ into 2 spaces; $$$R_1$$$, $$$R_2$$$
based on their classes $$$\omega_1$$$, $$$\omega_2$$$

================================================================================
Error cases of above example are following

- Bayes classifier incorrectly says given feature vector x (which is in class of $$$\omega_1$$$) is involved in $$$R_2$$$
- Bayes classifier incorrectly says given feature vector x (which is in class of $$$\omega_2$$$) is involved in $$$R_1$$$

================================================================================
Above error cases are exclusive to each other, that is, $$$\text{case1} \cap \text{case2} = \phi$$$

So, you can use summation on probability values

$$$P[\text{error}] \\
=P[\text{error}|\omega_1]P[\omega_1]+P[\text{error}|\omega_2]P[\omega_2]+\cdots+P[\text{error}|\omega_i]P[\omega_i] \\
=\sum\limits_{i=1}^{C} P[\text{error}|\omega_i] P[\omega_i]$$$

================================================================================
$$$P[\text{error}|\omega_i] \\
= P[\text{choose } \omega_j|\omega_i] \\
= \int_{R_j} P(x|\omega_i)dx$$$

* $$$P[\text{error}|\omega_i]$$$: probability of error occuring when $$$\omega_1$$$ is given

* $$$P[\text{choose } \omega_j|\omega_i]$$$: probability of "$$$\text{choose} \; \omega_j$$$" occuring when $$$\omega_1$$$ is given

* $$$\int_{R_j} P(x|\omega_i)dx$$$: Sum of probability of x occuring in $$$R_i$$$ area when $$$\omega_i$$$ is given

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/jjpr/pic/2019_04_12_08:56:07.png' alt=''><xmp>

* Center vertical line: decision line
* Colored area: probability of error that $$$x$$$ is misclassified into $$$R_2$$$
* $$$\int_{R_j} P(x|\omega_i)dx$$$

================================================================================
In conclusion, probability of error on 2 classes is following

$$$P[\text{error}]=P[\omega_1] \int_{R_2} P(x|\omega_1) dx + P[\omega_2] \int_{R_1} P(x|\omega_2) dx$$$

================================================================================
Let's say
$$$\int_{R_2} P(x|\omega_1) dx = \epsilon_1$$$
$$$\int_{R_1} P(x|\omega_2) dx = \epsilon_2$$$

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/mltheory/master/jjpr/pic/2019_04_12_09:12:45.png' alt=''><xmp>

* Why do you divide by 2? Because you had supposed same prior probability

================================================================================
In any cases, minimum probability of error is obtained via LRT decision rule.

Minimum probability of error (which is obtained via LRT decision rule) is called 
Bayes Error Rate, and classifier which uses Bayes Error Rate becomes best classifier.

================================================================================

</xmp>
   </BODY>
</HTML>
