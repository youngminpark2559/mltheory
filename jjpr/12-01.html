<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
},
img {
 width=900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
12-01 Dimentionality reduction by PCA(principle component analysis)
<xmp>
@
PCA : one of techniques of reducing dimensionality of feature vector

@
Generally, feature vector has various degrees and high dimensionality
However, you can run into various issues when working with above kind of vector,
so that there are many cases which require you to reduce dimensionality of feature vector

curse of dimensionality :
The higher dimensionality becomes, the more complex task becomes
This term is common designation for all issues which are occurred as dimensionality increases when you perform multivariate analysis

@
Let's suppose you have one patter recognition question where you should classify data into 3 classes
And let's think of simplest solution
1. You divide feature space as specific bin
2. You find ratio of each class' samples which are involved into each bin
3. When you run into unknown sample, you first find relevant bin, and you choose fittest class in that bin

Above method is bayes classifier which uses k-NNR,
that you find $$$\frac{k_{i}}{k}$$$, you assign unknown sample into largest i class

When you use bayes classifier, you find corresponding probability dense function by k-NNR method

=======================================================
When you use "single feature (color)", "3 classes (red, green, blue)"
=======================================================
2018-06-04 08-13-00.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-13-00.png"><xmp>
You can plot values (circles, rectangles, triangles) of feature onto 1 dimension axis
You can divide data (circles, rectangles, triangles) into each bin (3 bins in this case)
Then, you find dominat value in each bin

1st bin : red dominant
2nd bin : green dominant
3rd bin : blue dominant

If you have unknown sample in 2nd bin, you need to decide that sample would be involved in green dominant bin

When you use single feature, if you divide 1 dimension axis into 3 bins, you can see classes are overlapped in too many places

=======================================================
When you use "double features (feature $$$x_{1}$$$, feature $$$x_{2}$$$)", 
"3 classes for feature $$$x_{1}$$$, 3 classes for feature $$$x_{2}$$$"
=======================================================
When you use "double features (feature $$$x_{1}$$$, feature $$$x_{2}$$$)",
feature space becomes 2 dimension, number of bin increases up to 9 units

And you should choose one of following two methods (constant density, constant number of example)

constant density : 
2018-06-04 08-24-29.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-24-29.png"><xmp>
you make number of sample in each bin similar or constant
3 3 3
3 3 3
3 3 3
you need 27 samples in this case

constant number of example : 
2018-06-04 08-24-45.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-24-45.png"><xmp>
you keep original entire sample data
You need 9 samples in this case
You will run into "sparse 2 dimensional variance plot" which has zero or single sample in some bins,
which means you can't have methods which use samples in probabilistic method

=======================================================
When you use "triple features (feature $$$x_{1}$$$, feature $$$x_{2}$$$, feature $$$x_{3}$$$)", 
"3 classes for feature $$$x_{1}$$$, 3 classes for feature $$$x_{2}$$$, 3 classes for feature $$$x_{3}$$$"
=======================================================
2018-06-04 08-30-46.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-30-46.png"><xmp>
Problem becomes more serious

Number of bin should be $$$27=3^{3}$$$

constant density :
You need 81=3*27 samples

constant number of example :
You will run into very sparse 3 dimensional feature space


=======================================================
Issues as dimensionality of vector increases
=======================================================
1. Low performance of classification due to noise features
1. Slow speed of training and recognition
1. Huge size of train dataset which is required for modeling pridicting machine learning network

=======================================================
curse of the dimensionality :
=======================================================
2018-06-04 08-36-05.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-36-05.png"><xmp>
As dimensionality of feature vector increases, performance increases
However, at some point, performance decreases as dimensionality of feature vector increases

=======================================================
Solution for curse of the dimensionality :
=======================================================
1. You use prior knowledge 
1. You increase smoothness of target function (hypothesis function)
1. You reduce dimensionality of feature vector

=======================================================
How to reduce dimensionality of feature vector :
2 ways (feature selection, feature extraction)
=======================================================
Why only 2 ways?
Since features are composed as vector, 2 ways are methods which reduce "length of vector" in linear algebra way

@
1. feature selection : 
2018-06-04 08-43-08.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-43-08.png"><xmp>
You select subset of features from entire superset features
You include some kind of features, exclude some kind of features

You can reduce dimensionality from N dimension to M dimension

This way doesn't show deep implication

@
1. feature extraction : 
2018-06-04 08-50-52.png
</xmp><img src="https://raw.githubusercontent.com/youngmtool/mltheory/master/jjpr/pic/2018-06-04 08-50-52.png"><xmp>
You input existing feature vector into function f,
then you obtain subset which is composed of new features ($$$y_{1}, ..., y_{M}$$$),
which come from combination of existing features ($$$x_{1}, ..., x_{N}$$$)

Feature extraction is about finding function f

But with subset consists of new features y shouldn't lose characteristic of previous feature x

Generally, optimal mappring function f can be non linear function
However, mathematical way to find this non linear function is not known so far
So, all methods which are used in pattern recognition should be searched only from linear functions

=======================================================
Linear transformation
=======================================================
When y and x are vectors, the only way to perform linear transformation of x is to multiply "matrix W"

Linear transformation : 
You transforms x into y by multiplying some "matrix W"

You goal is to find this "matrix W"
To find this "matrix W", one of best ways is PCA methodology
      
</xmp>
   </BODY>
</HTML>
