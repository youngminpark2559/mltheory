<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 150px;
    margin-left: 40px;
    
    padding-top: 50px;
    padding-right: 30px;
    padding-bottom: 50px;
    padding-left: 40px;
    
    line-height:1.5em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                   displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>

딥러닝 2. 선형회귀와 Gradient Descent <br/>
Linear :<br/>
상수곱에 차수가 일차.<br/>
일차함수.<br/>
<br/>
Regression :<br/>
Line fitting for the graph on the hyperplane. Anyway, regression is fitting.<br/>
Linear Regression :<br/>
일차함수를 사용해 데이터를 fitting 하는 과정.<br/>
<br/>
학습시키는 과정 :<br/>
Regression 기술을 행하는 과정.<br/>
<br/>
y = a x + b<br/>
두 상수 a( 기울기 ), b( y절편 )에 의해 그래프의 특성이 결정된다. <br/>
상수튜닝을 어떻게 하냐, 상수들을 어떻게 최적화를 해나가냐, 하는게 linear regression의 핵심이다.<br/>
<br/>
@<br/>
W : weight, weight is nothing but a constant value.<br/>
<br/>
@<br/>
$y = W_{1}x + W_{2}$<br/>
이러한 직선을 regression 해야한다. <br/>
가장 기본적인 regression 방법은 gradient descent 방법이다.<br/>
사실 gradient descent가 딥러닝에서 가장 중요한 핵심이다. 시작과 끝이라는 표현도 과언이 아니다.<br/>
<br/>
@<br/>
집값 데이터 예시.<br/>
집넓이  집값(만원)<br/>
10    5000 <br/>
15    6500<br/>
16    7200<br/>
...<br/>
60    30,000<br/>
내가 사려고하는 집은 $26m^{2}$ 인데 가격을 예측하고싶다. <br/>
make sense 한 방법. 위 데이터로 그래프 그린다. x = 26 인 y 값을 읽으면 된다.<br/>
<br/>
@<br/>
hypothesis function y를 다음과 같이 설정한다.<br/>
$y = W_{1} x + W_{2}$<br/>
gradient descent라는 것은 hypothesis function으로 부터 파생된 cost function을 최소로 만들어주는 hypothesis <br/>function의 weight들인 $W{1}, W_{2}$ 상수들을 찾는 알고리즘이다. <br/>
$W_{1}, W_{2}$ 에 랜덤한 초기값을 일단 대입한다. 그러면 특정 hypothesis function의 그래프가 얻어질것이다.<br/>
실제데이터의 (x, y)값과 hypothesis function이 예측한 값 (x, y')의 오차(에러)를 구한뒤 제곱을 한다. 그리고 <br/>모든 데이터에 대해서 전부 구한뒤 다 더한다. 그리고 그 오차(에러)를 cost 라 정의한다. <br/>
틀리면 틀릴수록 비용, 즉 손해를 많이 본다는 얘기다. <br/>
집값이 5000 인줄 알았는데 실제로 6000 이다. 그럼 1000만원을 더 벌어야하기때문에 시간적 낭비가 초래되며  <br/>추가적인 고생도 하게된다. <br/>
cost 를 최소화하는 방향으로 hypothesis function의 $W_{1}, W_{2}$ 를 업데이트 해나갈것이다. <br/>
<br/>
@<br/>
linear regression 모델에서 hypothesis fuction $y = w_{1}x+w_{2}$ 에 대해 $y-y'$ 을 해서 나오는 mean square cost function $C = \sum\frac{1}{2m}(y-y')^{2}$<br/>
@<br/>
$W_{1}, W_{2}$ 를 변화시키면서 cost function $C = \sum\frac{1}{2m}(y-y')^{2}$<br/>
의 그래프를 그리면 이차함수모양의 그래프가 얻어진다.<br/>
<br/>
@<br/>
일단, hypothesis function $y = W_{1}x + W_{2} $ 에서 기울기를 의미하는 $W_{1}$ 만을 업데이트하면서 <br/>살펴본다고 가정한다. <br/>
cost function C가 최소가 될때 $W_{1}$ 이 얼마냐 가 관심사이다. <br/>
cost function을 미분했을때 0이되는 점이 cost function이 최소가 되는 지점이라고 볼수 있다.. <br/>
$W_{1}$에 관하여 cost function C를 미분한것이 0인 지점 혹은 $W_{1}$에서 cost function의 접선의 기울기가 0인 <br/>지점을 의미하는,<br/>
$\frac{dC}{dW_{1}}$ = 0 인 점을 찾아가는 것이다. <br/>
@<br/>
$ W_{1} := W_{1} - \frac{\partial f_{c}}{\partial W_{1}} $<br/>
식의 의미: $W_{1}$에 관하여 cost function C의 기울기값을 구한뒤, 이 기울기값과 $W_{1}$를 뺀다. 그 결과를 $W_<br/>{1}$에 새롭게 assign해서 $W_{1}$을 업데이트 시킨다.<br/>
$\frac{d}{dx}$는 x에 관하여 미분하는 것이다.<br/>
어떤 function의 정의역이 x에 대해서만 정의가 되어있을 때 사용하면 좋다. <br/>
예를들어, y가 x에 대한 함수, $y = ax+b$<br/>
<br/>
그런데 만약 미지수가 다양할때, $y = a_{1}x + a_{2}z$ 와 같은 경우에 편미분 쓰면 좋다. <br/>
x에 관하여 y를 편미분을 해라(x를 제외한 다른것은 상수로 보고 x에 관하여 미분), $\frac{\partial{y}}{\partial<br/>{x}} = a_{1}$<br/>
@<br/>
$W_{2} := W_{2} - \frac{\partial{C}}{\partial{W_{2}}}$<br/>
@<br/>
learning rate $\alpha$ 사용.<br/>
$W_{1} := W_{1}-\alpha\frac{\partial{C}}{\partial{W_{1}}}$<br/>
$W_{2} := W_{2}-\alpha\frac{\partial{C}}{\partial{W_{2}}}$<br/>
@<br/>
learning rate $\alpha$ 은 스케일 벡터라고 생각하면 된다.<br/>
learning rate 을 튜닝을 잘 해서 cost function에서 gradient descent를 할때 발산할걸 안쪽으로 오게할수있고, <br/>gradient descent가 너무 느린걸 leaning rate 을 좀 키워서 더 많이 가게 세팅할수도있다. <br/>
<br/>
@<br/>
hypothesis function y가 <br/>
y = $W_{1}x + W_{2}$ 일때,<br/>
cost function의 최소값을 찾기위해,<br/>
$W_{1}$ 을 업데이트하면 hypothesis function의 기울기보정<br/>
$W_{2}$ 을 업데이트하면 hypothesis function의 y절편, bias 를 조정.<br/>
cost function에서 gradient descent를 할 때, 이 두 값이 동시에 계속 조정된다. line 을 fitting 해 나간다. <br/>
      
   </BODY>
</HTML>
