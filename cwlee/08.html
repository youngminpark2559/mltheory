<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
    font-size: 25px;
 
    margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 75px;
    margin-left: 75px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 75px;
    padding-left: 75px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                   displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
   
@Chapter<br/>
08. softmax function<br/>
<br/>
@<br/>
softmax function $y = \frac{e^{wx}}{\sum{e^{wx}}}$ <br/>
<br/>
sigmoid function이랑 비슷한 성질을 가지고 있다. activation function 이라고 생각하면 된다. <br/>
<br/>
@<br/>
softmax function 의 역할을 알아보기위해 간단한 NN을 그려보자.<br/>
<br/>
input : $x_{1}, x_{2}$<br/>
activation functions ( in hidden layer ) : 노드 3개<br/>
output layer : 노드 6개<br/>
<br/>
@<br/>
현재까지 hidden layer와 output layer 에서 sigmoid activation function 을 사용해 왔었다. 거기서 cost function 를 구하고 gradient descent로 기울기를 구해서 hypothesis function의 weight를 업데이트 시키는 방법으로 hypothesis function의 line을 fitting 시키시면서 학습을 시켜왔다. <br/>
<br/>
그런데 과학자들이 연구를 하다보니까 output layer에는 softmax function 사용했을 때 output layer 에 sigmoid function을 쓰는것보다 성능이 더 잘나온다 라는 결론이 나왔다. 그래서 이제부터는 output layer 에는 왠만하면 sigmoid function보다 softmax function 을 쓰도록 하자 라는 방법론이 제기가 되었다. <br/>
<br/>
@<br/>
우리가 해 볼것은. output layer에 sigmoid function을 사용할 때와 softmax function을 사용할때 어떤 차이가 있나 확인해보자. <br/>
<br/>
@<br/>
우선 output layer 에서 sigmoid function을 적용한것과 softmax function을 적용한 것 모두에서 각각의 클래스(output layer에서 output 값 y를 출력하는 노드)들은 [0,1] 범위의 output 값을 출력한다는 것은 동일하다. <br/>
<br/>
그런데 softmax function을 보면 알겠지만, <br/>
$y = \frac{e^{wx}}{\sum{e^{wx}}}$<br/>
분모가 $e^{wx}$ 의 합($\sum{e^{wx}}$)이다. <br/>
예를들어, output layer Y의 모든 노드에는 $y = \frac{e^{wx}}{\sum{e^{wx}}}$ 이와 같은 softmax function이 있게 되고, output 값인 클래스를 출력하게 되는데, $y_{1} = 0.7$ 이면 $y_{2}, ..., y_{6}$ 까지 클래스들이 0.3을 나눠갖아야한다.<br/>
<br/>
정규분포 이런데서 얘기하는 정규화 개념과 비슷하다고 보면 된다. <br/>
<br/>
@<br/>
왜 굳이 1을 만드는지 궁금할수도 있다. 장점이 있으니까 이렇게 쓰겠지. <br/>
<br/>
sigmoid function 같은 경우는 각각 독립적인 역할을 하는 activation function 이기 때문에 특정 노드의 activation function 이 변한다고 해서 다른 노드들의 output 출력값에 영향을 주지 않는다. 각 노드들에 의해 독립적으로 output 값들이 계산이 되고, 각각 독립적인 cost 들을 가지게 된다. 이말은 곧 특정 노드는 오로지 특정 노드로 input data 를 준 hidden layer의 노드들에 관련된는 weight 만 업데이트가 가능하다. 노드들이 local 에서 weight를 각각 업데이트 시키는 것이다. back propagation으로 한번 업데이트 하고 끝이다. <br/>
<br/>
@<br/>
output layer에 softmax function을 쓰더라도 cost 가 줄어드는 쪽으로 weight들이 업데이트 되면서 output layer에서 출력하는 output 값들이 계속 변화한다. 예를들어, $y_{1}$ 이 고양이라는 클래스일때, input으로 고양이 사진을 주었다면 $y_{1}$ 값이 높아지게 된다. 그러면 sigmoid function 같은 경우는 $y_{1}$이 높아지면 $y_{2}, ..., y_{6}$ 은 독립적으로 떨어뜨린다. softmax function같은경우는 $y_{1}$ 이 계속 높아져서 1로 갈때, softmax function의 출력값의 합은 1이 라는 softmax function의 특성때문에 $y_{2}, ..., y_{6}$ 의 output 값들은 반드시 떨어져야한다. output 값 $y_{2}, ..., y_{6}$ 들이 떨어진다는 것은 output layer 의 $Y_{2}, ..., Y_{6}$ 들의 softmax function에서의 weight 값들이 조정되어서 output 값들이 떨어지는 것이다. 즉, $Y_{1}$ 의 weight가 업데이트 되어 출력을 1에 근접하게 만들게 되면 이 노드의 상태가 다른 output layer의 노드들의 상태, weight들을 조절, 관여할 수 있다는 것이다. 학습이 더 빨라지는 효과가 있다. <br/>
<br/>



      
   </BODY>
</HTML>
