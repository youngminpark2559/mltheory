<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
img {
 width:900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
<xmp>
==================================================
Optimization problem is to find variable $$$x^*$$$  
which makes value of f minimum or maximum  

$$$x^*=arg_x max f(x)$$$  
$$$x^*=arg_x min f(x)$$$  

$$$x^*$$$ is called solution to the optimization problem  

==================================================
Since you can automatically solve max optimization problem,
by adding $$$-$$$ like $$$-f(x)$$$,
you generally should consider only min optimization problem. 

Function $$$f(x)$$$ which you want to minimize is called  
as objective function, cost function, loss function 

==================================================

Grid search by values:  
1. you continuously assign various values to x  
2. you see whether $$$f(x)$$$ is mimimum 

Grid search by graph:  
1. you draw graph of function  
2. you find x value which minimizes $$$f(x)$$$ 

==================================================
Numerical optimization:  
1. you perform trial and error  
(which means you move x value like gradient descent algorithm)  
2. you find $$$x^*$$$ which satisties necessary condition for optimization

==================================================
To do numerical optimization,  
you should need 2 algorithms  

1. algorithm to confirm whether current $$$x_k$$$ is optimal  
2. algorithm to find next position $$$x_{k+1}$$$  

==================================================
Algorithm to confirm whether current $$$x_k$$$ is optimal  
using slope information  

If $$$f(x)$$$ is N univariate function,  
$$$\dfrac{df(x)}{dx}=0$$$  

If $$$f(x)$$$ is multivariate function,  
$$$\dfrac{\partial f(x)}{\partial x_1}=0$$$  
$$$\dfrac{\partial f(x)}{\partial x_2}=0$$$  
$$$\vdots$$$  
$$$\dfrac{\partial f(x)}{\partial x_N}=0$$$  

In other words, $$$\triangledown f=0$$$ 

==================================================
Algorithm to find next position $$$x_{k+1}$$$  
using SGD  

SGD finds $$$x_{k+1}$$$ by using slope information at $$$x_k$$$  

$$$x_{k+1} \\ 
=x_k - \alpha \times \triangledown f(x_k) \\ 
=x_k - \alpha g(x_k)$$$

==================================================
Newton method using second order derivative  

Suppose:  
objective function should be second order function  
Then, you can find optimal point $$$x_k$$$ by one try  

Method:  
Use gradient vector which is chaned in direction and length  
which can be obtained by multiplying gradient vector 
by inverse matrix of Hessian matrix  

$$${x}_{n+1} = {x}_n - [{H}f({x}_n)]^{-1} \times \nabla f({x}_n)$$$  

Meaning:  
1. You don't need learning rate  
2. You need one order derivative for gradient vector  
2. You need second order derivative for Hessian matrix  

==================================================
Quasi-Newton method  

==================================================
Global optimization problem  
If f(x) has multiple local minima,  
you can assure numerical optimization method reaches to global minimum 

Numerical optimization method depends on initial value, algorithm, parameter, etc  

==================================================
Convex problem  
Convex problem is optimization problem 
which is defined in the range
where second order derivative is greater or equal to 0  

$$$\dfrac{\partial^2 f}{\partial x^2} \ge 0$$$ 

If f(x) is multivariate function  
Hessian matrix should always be "positive semidefinite"  
in given range  

$$$x^T \times \text{(Hessian mat)} \times x \ge 0$$$, $$$\forall$$$ x  

Convex problem gurantees there exist global optimal point  

</xmp>
   </BODY>
</HTML>
